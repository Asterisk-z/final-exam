{
  "version": 4,
  "terraform_version": "1.4.0",
  "serial": 29,
  "lineage": "2c90c68e-4cbc-c20d-c7b5-b1f7968bfd72",
  "outputs": {},
  "resources": [
    {
      "mode": "data",
      "type": "aws_eks_cluster",
      "name": "eks-cluster",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:eks:us-east-1:366336511452:cluster/eks-cluster",
            "certificate_authority": [
              {
                "data": "LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMvakNDQWVhZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJek1EUXdOekExTURJeE1Gb1hEVE16TURRd05EQTFNREl4TUZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBUEFZCnFaQmhkeURzL0t2bTNwQ1dHTU5pcTZmaHZqZ05MNTh6OTZJUjhNaXFUK1RNZk9haWQydG1yNHdkb0RqWGpZYkUKanFPODhoL1h1cDlEZUZHN0d6bjdKR05JWW5uK0sxeExYZ2x0TWE5MjFUVUQwZEtpWXV1Z296cTVxU2pvYW1OcgpNOHZjZmo5am83M0xVd2x1SytvMTNlZDY4eDMyK2V2ZjBYN2lyU3hVQXZoVlh0L0s5NWFXYzBCa3FocGUrSWNTCjVmSzVBaDhuMFhNK2J0WlpLczB4SDRSaldvVHBDcVJ5dzJpeXhuWlYwdHpaalE5d2REVDdQVjR4cXVaWWRqUnYKR2pBOTR4TXBGVjhZbXlkSGxmQWVESi9BS2M3SXh5ekRKeEZFQk1mZk8rVmNFZ2NlTmZWRG1YUllQM2RFRjR1cwpwc2FJbmUrZkljT0Z0dkxGQmNNQ0F3RUFBYU5aTUZjd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZBREQ1anMxYU1BT015QlhFaGRFMVRkanpsRWdNQlVHQTFVZEVRUU8KTUF5Q0NtdDFZbVZ5Ym1WMFpYTXdEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUJBSVhyd2djdm1UMmlRa2VFdHdqZQp1RmttR0hTZld0ZHh6RkdiQ0s0YjkyZWkxODdUMXpnU3RqUzRoSVlMMGJXMy95MHMyNVI3TzY0WEdPYW9NMWdJCi80VG15dUdhd3d4NU81VHJ1aU5aSjAzQ2ZRZUhaYUlmMWVxak00Q09wbFlkN2x2MjlkeDFKQnZ3S1RLOVhBNzUKa0RNWi8zM0RYb1U2UzB1cjNISms4MGhsa0ZOZGtKUFo3MWt2Y1pLc1dLdS9lQW5NanVtcVMzdEg0eDBBNlY5WgpISzJZQXkza25CTzI4MWhwN3FoVVB5RUZiWWNOMEg0YzI3R2RYcGdHL3Y2R0toK1hOalpxazNBZUsxUG5tc2RCCm5jSDRtazlHMmRSK2s3ZjZCR2thbi9EYWVBQ2E1SHVxdy9peXpqOVNRaURKT05LRjd4RVlzSVcya3BXNEpQb0sKR1dVPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg=="
              }
            ],
            "cluster_id": null,
            "created_at": "2023-04-07 04:56:24.086 +0000 UTC",
            "enabled_cluster_log_types": [
              "api",
              "audit"
            ],
            "endpoint": "https://76CEC5A46F8E4FF741EC9C71C928E247.gr7.us-east-1.eks.amazonaws.com",
            "id": "eks-cluster",
            "identity": [
              {
                "oidc": [
                  {
                    "issuer": "https://oidc.eks.us-east-1.amazonaws.com/id/76CEC5A46F8E4FF741EC9C71C928E247"
                  }
                ]
              }
            ],
            "kubernetes_network_config": [
              {
                "ip_family": "ipv4",
                "service_ipv4_cidr": "172.20.0.0/16",
                "service_ipv6_cidr": ""
              }
            ],
            "name": "eks-cluster",
            "outpost_config": [],
            "platform_version": "eks.1",
            "role_arn": "arn:aws:iam::366336511452:role/eks-cluster-role",
            "status": "ACTIVE",
            "tags": {},
            "version": "1.25",
            "vpc_config": [
              {
                "cluster_security_group_id": "sg-067fd87395521ae4e",
                "endpoint_private_access": false,
                "endpoint_public_access": true,
                "public_access_cidrs": [
                  "0.0.0.0/0"
                ],
                "security_group_ids": [
                  "sg-08497ef858cf5a836"
                ],
                "subnet_ids": [
                  "subnet-062ea5ee84ccc21cb",
                  "subnet-0bac09f0c8a686e49",
                  "subnet-0db45259b5ca9b38b",
                  "subnet-0e937e1f94ee796a0"
                ],
                "vpc_id": "vpc-0633efbe91dee0c5d"
              }
            ]
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "mode": "data",
      "type": "aws_eks_cluster_auth",
      "name": "eks-token",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "eks-cluster",
            "name": "eks-cluster",
            "token": "k8s-aws-v1.aHR0cHM6Ly9zdHMuYW1hem9uYXdzLmNvbS8_QWN0aW9uPUdldENhbGxlcklkZW50aXR5JlZlcnNpb249MjAxMS0wNi0xNSZYLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWS1MyM1JYT0hSWkI0UlJQJTJGMjAyMzA0MDclMkZ1cy1lYXN0LTElMkZzdHMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDIzMDQwN1QwNjM5MDZaJlgtQW16LUV4cGlyZXM9MCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QlM0J4LWs4cy1hd3MtaWQmWC1BbXotU2lnbmF0dXJlPTVlMzNlZDk3MDg5MzM5MGI3YzIyYWM0OWU4YWVlOTg0NDU0ZDcxMzQwZTk4ZGUyNTE3YWUzMDNjODdhOWYwZWY"
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "mode": "data",
      "type": "kubectl_path_documents",
      "name": "docs",
      "provider": "provider[\"registry.terraform.io/gavinbunney/kubectl\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "disable_template": false,
            "documents": [
              "apiVersion: v1\nkind: Namespace\nmetadata:\n  name: monitoring",
              "---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: prometheus\n  namespace: monitoring\n  labels:\n    app: prometheus",
              "---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: prometheus\n  labels:\n    app: prometheus\nrules:\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - ingresses\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups: [\"\"] # \"\" indicates the core API group\n  resources:\n  - nodes\n  - nodes/metrics\n  - nodes/proxy\n  - services\n  - endpoints\n  - pods\n  verbs:\n  - get\n  - list\n  - watch\n- nonResourceURLs:\n  - /metrics\n  verbs:\n  - get",
              "---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: prometheus\n  labels:\n    app: prometheus\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: prometheus\nsubjects:\n- kind: ServiceAccount\n  name: prometheus\n  namespace: monitoring",
              "apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: prometheus-configmap\n  namespace: monitoring\ndata:\n  prometheus.yml: |\n    global:\n      scrape_interval: 15s\n    rule_files:\n      - \"/etc/prometheus-rules/alert.rules\"\n    alerting:\n      alertmanagers:\n      - static_configs:\n        - targets:\n          - alertmanager:9093\n    scrape_configs:\n    - job_name: 'kubernetes-apiservers'\n      kubernetes_sd_configs:\n      - role: endpoints\n      scheme: https\n      tls_config:\n        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n      relabel_configs:\n      - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]\n        action: keep\n        regex: default;kubernetes;https\n    - job_name: 'kubernetes-nodes'\n      scheme: https\n      tls_config:\n        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n      kubernetes_sd_configs:\n      - role: node\n      relabel_configs:\n      - action: labelmap\n        regex: __meta_kubernetes_node_label_(.+)\n      - target_label: __address__\n        replacement: kubernetes.default.svc:443\n      - source_labels: [__meta_kubernetes_node_name]\n        regex: (.+)\n        target_label: __metrics_path__\n        replacement: /api/v1/nodes/1/proxy/metrics\n    - job_name: 'kubernetes-cadvisor'\n      scheme: https\n      tls_config:\n        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n      kubernetes_sd_configs:\n      - role: node\n      relabel_configs:\n      - action: labelmap\n        regex: __meta_kubernetes_node_label_(.+)\n      - target_label: __address__\n        replacement: kubernetes.default.svc:443\n      - source_labels: [__meta_kubernetes_node_name]\n        regex: (.+)\n        target_label: __metrics_path__\n        replacement: /api/v1/nodes/1/proxy/metrics/cadvisor\n    - job_name: 'kubernetes-service-endpoints'\n      kubernetes_sd_configs:\n      - role: endpoints\n      relabel_configs:\n      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]\n        action: keep\n        regex: true\n      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]\n        action: replace\n        target_label: __scheme__\n        regex: (https?)\n      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]\n        action: replace\n        target_label: __metrics_path__\n        regex: (.+)\n      - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]\n        action: replace\n        target_label: __address__\n        regex: ([^:]+)(?::\\d+)?;(\\d+)\n        replacement: $1:$2\n      - action: labelmap\n        regex: __meta_kubernetes_service_label_(.+)\n      - source_labels: [__meta_kubernetes_namespace]\n        action: replace\n        target_label: kubernetes_namespace\n      - source_labels: [__meta_kubernetes_service_name]\n        action: replace\n        target_label: kubernetes_name\n    - job_name: 'kubernetes-services'\n      metrics_path: /probe\n      params:\n        module: [http_2xx]\n      kubernetes_sd_configs:\n      - role: service\n      relabel_configs:\n      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]\n        action: keep\n        regex: true\n      - source_labels: [__address__]\n        target_label: __param_target\n      - target_label: __address__\n        replacement: blackbox-exporter.example.com:9115\n      - source_labels: [__param_target]\n        target_label: instance\n      - action: labelmap\n        regex: __meta_kubernetes_service_label_(.+)\n      - source_labels: [__meta_kubernetes_namespace]\n        target_label: kubernetes_namespace\n      - source_labels: [__meta_kubernetes_service_name]\n        target_label: kubernetes_name\n    - job_name: 'kubernetes-pods'\n      kubernetes_sd_configs:\n      - role: pod\n      relabel_configs:\n      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]\n        action: keep\n        regex: true\n      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]\n        action: replace\n        target_label: __metrics_path__\n        regex: (.+)\n      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]\n        action: replace\n        regex: ([^:]+)(?::\\d+)?;(\\d+)\n        replacement: $1:$2\n        target_label: __address__\n      - action: labelmap\n        regex: __meta_kubernetes_pod_label_(.+)\n      - source_labels: [__meta_kubernetes_namespace]\n        action: replace\n        target_label: kubernetes_namespace\n      - source_labels: [__meta_kubernetes_pod_name]\n        action: replace\n        target_label: kubernetes_pod_name\n    - job_name: 'kubernetes-ingresses'\n      kubernetes_sd_configs:\n      - role: ingress\n      relabel_configs:\n      - source_labels: [__meta_kubernetes_ingress_annotation_prometheus_io_probe]\n        action: keep\n        regex: true\n      - source_labels: [__meta_kubernetes_ingress_scheme,__address__,__meta_kubernetes_ingress_path]\n        regex: (.+);(.+);(.+)\n        replacement: 1://23\n        target_label: __param_target\n      - target_label: __address__\n        replacement: blackbox-exporter.example.com:9115\n      - source_labels: [__param_target]\n        target_label: instance\n      - action: labelmap\n        regex: __meta_kubernetes_ingress_label_(.+)\n      - source_labels: [__meta_kubernetes_namespace]\n        target_label: kubernetes_namespace\n      - source_labels: [__meta_kubernetes_ingress_name]\n        target_label: kubernetes_name\n    - job_name: 'kube-state-metrics'\n      static_configs:\n        - targets: ['kube-state-metrics.monitoring.svc.cluster.local:8080']\n    - job_name: 'node-exporter'\n      kubernetes_sd_configs:\n      - role: endpoints\n      relabel_configs:\n      - source_labels: [__meta_kubernetes_endpoints_name]\n        regex: 'node-exporter'\n        action: keep",
              "# Useful examples on how to configure Prometheus\n# * https://www.weave.works/prometheus-and-kubernetes-monitoring-your-applications/\n# * https://grafana.net/dashboards/162\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: prometheus-alertrules\n  namespace: monitoring\ndata:\n  alert.rules: |-\n    groups:\n    - name: HighErrorRate\n      rules:\n      - alert: HighErrorRate\n        expr: rate(request_duration_seconds_count{status_code=\"500\"}[5m]) \u003e 1\n        for: 5m\n        labels:\n          severity: slack\n        annotations:\n          summary: \"High HTTP 500 error rates\"\n          description: \"Rate of HTTP 500 errors per 5 minutes: {{ $value }}\"",
              "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus-deployment\n  namespace: monitoring\nspec:\n  replicas: 1\n  strategy:\n    rollingUpdate:\n      maxSurge: 0\n      maxUnavailable: 1\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: prometheus\n  template:\n    metadata:\n      name: prometheus\n      labels:\n        app: prometheus\n    spec:\n      serviceAccount: prometheus\n      containers:\n      - name: prometheus\n        image: prom/prometheus:v2.26.0\n        args:\n        - '--storage.tsdb.retention=360h'\n        - '--config.file=/etc/prometheus/prometheus.yml'\n        ports:\n        - name: web\n          containerPort: 9090\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/prometheus\n        - name: alertrules-volume\n          mountPath: /etc/prometheus-rules\n      volumes:\n      - name: config-volume\n        configMap:\n          name: prometheus-configmap\n      - name: alertrules-volume\n        configMap:\n          name: prometheus-alertrules\n      nodeSelector:\n        beta.kubernetes.io/os: linux",
              "apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    prometheus.io/scrape: 'true'\n  labels:\n    name: prometheus\n  name: prometheus\n  namespace: monitoring\nspec:\n  selector:\n    app: prometheus\n  type: LoadBalancer\n  ports:\n  - name: prometheus\n    protocol: TCP\n    port: 9090\n    targetPort: 9090",
              "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-directory-size-metrics\n  namespace: monitoring\n  annotations:\n    description: |\n      This `DaemonSet` provides metrics in Prometheus format about disk usage on the nodes.\n      The container `read-du` reads in sizes of all directories below /mnt and writes that to `/tmp/metrics`. It only reports directories larger then `100M` for now.\n      The other container `caddy` just hands out the contents of that file on request via `http` on `/metrics` at port `9102` which are the defaults for Prometheus.\n      These are scheduled on every node in the Kubernetes cluster.\n      To choose directories from the node to check, just mount them on the `read-du` container below `/mnt`.\nspec:\n  selector:\n    matchLabels:\n      app: node-directory-size-metrics\n  template:\n    metadata:\n      labels:\n        app: node-directory-size-metrics\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9102'\n        description: |\n          This `Pod` provides metrics in Prometheus format about disk usage on the node.\n          The container `read-du` reads in sizes of all directories below /mnt and writes that to `/tmp/metrics`. It only reports directories larger then `100M` for now.\n          The other container `caddy` just hands out the contents of that file on request on `/metrics` at port `9102` which are the defaults for Prometheus.\n          This `Pod` is scheduled on every node in the Kubernetes cluster.\n          To choose directories from the node to check just mount them on `read-du` below `/mnt`.\n    spec:\n      containers:\n      - name: read-du\n        image: giantswarm/tiny-tools\n        imagePullPolicy: Always\n        # FIXME threshold via env var\n        # The\n        command:\n        - fish\n        - --command\n        - |\n          while true\n            for directory in (du --bytes --separate-dirs --threshold=100M /mnt)\n              echo $directory | read size path\n              echo \"node_directory_size_bytes{path=\\\"$path\\\"} $size\" \\\n                \u003e\u003e /tmp/metrics-temp\n            end\n            mv /tmp/metrics-temp /tmp/metrics\n            sleep 300\n          end\n        volumeMounts:\n        - name: host-fs-var\n          mountPath: /mnt/var\n          readOnly: true\n        - name: metrics\n          mountPath: /tmp\n      - name: caddy\n        image: dockermuenster/caddy:0.9.3\n        command:\n        - \"caddy\"\n        - \"-port=9102\"\n        - \"-root=/var/www\"\n        ports:\n        - containerPort: 9102\n        volumeMounts:\n        - name: metrics\n          mountPath: /var/www\n      volumes:\n      - name: host-fs-var\n        hostPath:\n          path: /var\n      - name: metrics\n        emptyDir:\n          medium: Memory",
              "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: 2.1.0\n  name: kube-state-metrics\n  namespace: monitoring",
              "apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: 2.1.0\n  name: kube-state-metrics\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - configmaps\n  - secrets\n  - nodes\n  - pods\n  - services\n  - resourcequotas\n  - replicationcontrollers\n  - limitranges\n  - persistentvolumeclaims\n  - persistentvolumes\n  - namespaces\n  - endpoints\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - apps\n  resources:\n  - statefulsets\n  - daemonsets\n  - deployments\n  - replicasets\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - batch\n  resources:\n  - cronjobs\n  - jobs\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - autoscaling\n  resources:\n  - horizontalpodautoscalers\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - authentication.k8s.io\n  resources:\n  - tokenreviews\n  verbs:\n  - create\n- apiGroups:\n  - authorization.k8s.io\n  resources:\n  - subjectaccessreviews\n  verbs:\n  - create\n- apiGroups:\n  - policy\n  resources:\n  - poddisruptionbudgets\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - certificates.k8s.io\n  resources:\n  - certificatesigningrequests\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - storage.k8s.io\n  resources:\n  - storageclasses\n  - volumeattachments\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - admissionregistration.k8s.io\n  resources:\n  - mutatingwebhookconfigurations\n  - validatingwebhookconfigurations\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - networkpolicies\n  - ingresses\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - coordination.k8s.io\n  resources:\n  - leases\n  verbs:\n  - list\n  - watch",
              "apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: 2.1.0\n  name: kube-state-metrics\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: kube-state-metrics\nsubjects:\n- kind: ServiceAccount\n  name: kube-state-metrics\n  namespace: monitoring",
              "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: 2.1.0\n  name: kube-state-metrics\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: 2.1.0\n    spec:\n      containers:\n      - image: k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.1.0\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        name: kube-state-metrics\n        ports:\n        - containerPort: 8080\n          name: http-metrics\n        - containerPort: 8081\n          name: telemetry\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        securityContext:\n          runAsUser: 65534\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: kube-state-metrics",
              "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: 2.1.0\n  name: kube-state-metrics\n  namespace: monitoring\nspec:\n  clusterIP: None\n  ports:\n  - name: http-metrics\n    port: 8080\n    targetPort: http-metrics\n  - name: telemetry\n    port: 8081\n    targetPort: telemetry\n  selector:\n    app.kubernetes.io/name: kube-state-metrics",
              "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grafana-core\n  namespace: monitoring\n  labels:\n    app: grafana\n    component: core\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: grafana\n      component: core\n  template:\n    metadata:\n      labels:\n        app: grafana\n        component: core\n    spec:\n      containers:\n      - image: grafana/grafana:7.5.5\n        name: grafana-core\n        imagePullPolicy: IfNotPresent\n        # env:\n        resources:\n          # keep request = limit to keep this container in guaranteed class\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        env:\n          # The following env variables set up basic auth twith the default admin user and admin password.\n          - name: GF_SECURITY_ADMIN_USER\n            value: admin\n          - name: GF_SECURITY_ADMIN_PASSWORD\n            value: admin\n        readinessProbe:\n          httpGet:\n            path: /login\n            port: 3000\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: grafana-persistent-storage\n          mountPath: /var/lib/grafana\n      volumes:\n      - name: grafana-persistent-storage\n        emptyDir: {}\n      nodeSelector:\n        beta.kubernetes.io/os: linux",
              "apiVersion: v1\nkind: Service\nmetadata:\n  name: grafana\n  namespace: monitoring\n  labels:\n    app: grafana\n    component: core\nspec:\n  type: LoadBalancer\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 3000\n  selector:\n    app: grafana\n    component: core",
              "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: grafana-import-dashboards\n  namespace: monitoring\n  labels:\n    app: grafana\n    component: import-dashboards\nspec:\n  template:\n    metadata:\n      name: grafana-import-dashboards\n      labels:\n        app: grafana\n        component: import-dashboards\n      annotations:\n        pod.beta.kubernetes.io/init-containers: '[\n          {\n            \"name\": \"wait-for-endpoints\",\n            \"image\": \"giantswarm/tiny-tools\",\n            \"imagePullPolicy\": \"IfNotPresent\",\n            \"command\": [\"fish\", \"-c\", \"echo \\\"waiting for endpoints...\\\"; while true; set endpoints (curl -s --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt --header \\\"Authorization: Bearer \\\"(cat /var/run/secrets/kubernetes.io/serviceaccount/token) https://kubernetes.default.svc/api/v1/namespaces/monitoring/endpoints/grafana); echo $endpoints | jq \\\".\\\"; if test (echo $endpoints | jq -r \\\".subsets[].addresses | length\\\") -gt 0; exit 0; end; echo \\\"waiting...\\\";sleep 1; end\"],\n            \"args\": [\"monitoring\", \"grafana\"]\n          }\n        ]'\n    spec:\n      containers:\n      - name: grafana-import-dashboards\n        image: giantswarm/tiny-tools\n        command: [\"/bin/sh\", \"-c\"]\n        workingDir: /opt/grafana-import-dashboards\n        args:\n          - \u003e\n            for file in *-datasource.json ; do\n              if [ -e \"$file\" ] ; then\n                echo \"importing $file\" \u0026\u0026\n                curl --silent --fail --show-error \\\n                  --request POST http://admin:admin@grafana/api/datasources \\\n                  --header \"Content-Type: application/json\" \\\n                  --header \"Accept: application/json\" \\\n                  --data-binary \"@$file\" ;\n                echo \"\" ;\n              fi\n            done ;\n            for file in *-dashboard.json ; do\n              if [ -e \"$file\" ] ; then\n                echo \"importing $file\" \u0026\u0026\n                curl --silent --fail --show-error \\\n                  --request POST http://admin:admin@grafana/api/dashboards/import \\\n                  --header \"Content-Type: application/json\" \\\n                  --header \"Accept: application/json\" \\\n                  --data-binary \"@$file\" ;\n                echo \"\" ;\n              fi\n            done ;\n        volumeMounts:\n        - name: config-volume\n          mountPath: /opt/grafana-import-dashboards\n      restartPolicy: Never\n      volumes:\n      - name: config-volume\n        configMap:\n          name: grafana-import-dashboards",
              "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    app.kubernetes.io/component: exporter\n    app.kubernetes.io/name: node-exporter\n  name: node-exporter\n  namespace: monitoring",
              "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/component: exporter\n    app.kubernetes.io/name: node-exporter\n  name: node-exporter\n  namespace: monitoring\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: exporter\n      app.kubernetes.io/name: node-exporter\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: exporter\n        app.kubernetes.io/name: node-exporter\n    spec:\n      containers:\n      - args:\n        - --web.listen-address=0.0.0.0:9100\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --no-collector.wifi\n        - --no-collector.hwmon\n        - --collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/pods/.+)($|/)\n        - --collector.netclass.ignored-devices=^(veth.*)$\n        - --collector.netdev.device-exclude=^(veth.*)$\n        image: quay.io/prometheus/node-exporter:v1.1.2\n        name: node-exporter\n        resources:\n          limits:\n            cpu: 250m\n            memory: 180Mi\n          requests:\n            cpu: 102m\n            memory: 180Mi\n        volumeMounts:\n        - mountPath: /host/sys\n          mountPropagation: HostToContainer\n          name: sys\n          readOnly: true\n        - mountPath: /host/root\n          mountPropagation: HostToContainer\n          name: root\n          readOnly: true\n        ports:\n        - containerPort: 9100\n          hostPort: 9100\n          name: http\n      hostNetwork: true\n      hostPID: true\n      nodeSelector:\n        kubernetes.io/os: linux\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: node-exporter\n      tolerations:\n      - operator: Exists\n      volumes:\n      - hostPath:\n          path: /sys\n        name: sys\n      - hostPath:\n          path: /\n        name: root\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 10%\n    type: RollingUpdate",
              "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/component: exporter\n    app.kubernetes.io/name: node-exporter\n  name: node-exporter\n  namespace: monitoring\nspec:\n  clusterIP: None\n  ports:\n  - name: http\n    port: 9100\n    targetPort: 9100\n  selector:\n    app.kubernetes.io/component: exporter\n    app.kubernetes.io/name: node-exporter"
            ],
            "id": "a0e35b68e31c5c69bdba4cecfaa0aaa309cfbffb7a493e959d1e85ed06b3139b",
            "manifests": {
              "/api/v1/namespaces/monitoring": "apiVersion: v1\nkind: Namespace\nmetadata:\n  name: monitoring\n",
              "/api/v1/namespaces/monitoring/configmaps/prometheus-alertrules": "apiVersion: v1\ndata:\n  alert.rules: |-\n    groups:\n    - name: HighErrorRate\n      rules:\n      - alert: HighErrorRate\n        expr: rate(request_duration_seconds_count{status_code=\"500\"}[5m]) \u003e 1\n        for: 5m\n        labels:\n          severity: slack\n        annotations:\n          summary: \"High HTTP 500 error rates\"\n          description: \"Rate of HTTP 500 errors per 5 minutes: {{ $value }}\"\nkind: ConfigMap\nmetadata:\n  name: prometheus-alertrules\n  namespace: monitoring\n",
              "/api/v1/namespaces/monitoring/configmaps/prometheus-configmap": "apiVersion: v1\ndata:\n  prometheus.yml: |-\n    global:\n      scrape_interval: 15s\n    rule_files:\n      - \"/etc/prometheus-rules/alert.rules\"\n    alerting:\n      alertmanagers:\n      - static_configs:\n        - targets:\n          - alertmanager:9093\n    scrape_configs:\n    - job_name: 'kubernetes-apiservers'\n      kubernetes_sd_configs:\n      - role: endpoints\n      scheme: https\n      tls_config:\n        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n      relabel_configs:\n      - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]\n        action: keep\n        regex: default;kubernetes;https\n    - job_name: 'kubernetes-nodes'\n      scheme: https\n      tls_config:\n        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n      kubernetes_sd_configs:\n      - role: node\n      relabel_configs:\n      - action: labelmap\n        regex: __meta_kubernetes_node_label_(.+)\n      - target_label: __address__\n        replacement: kubernetes.default.svc:443\n      - source_labels: [__meta_kubernetes_node_name]\n        regex: (.+)\n        target_label: __metrics_path__\n        replacement: /api/v1/nodes/1/proxy/metrics\n    - job_name: 'kubernetes-cadvisor'\n      scheme: https\n      tls_config:\n        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n      kubernetes_sd_configs:\n      - role: node\n      relabel_configs:\n      - action: labelmap\n        regex: __meta_kubernetes_node_label_(.+)\n      - target_label: __address__\n        replacement: kubernetes.default.svc:443\n      - source_labels: [__meta_kubernetes_node_name]\n        regex: (.+)\n        target_label: __metrics_path__\n        replacement: /api/v1/nodes/1/proxy/metrics/cadvisor\n    - job_name: 'kubernetes-service-endpoints'\n      kubernetes_sd_configs:\n      - role: endpoints\n      relabel_configs:\n      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]\n        action: keep\n        regex: true\n      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]\n        action: replace\n        target_label: __scheme__\n        regex: (https?)\n      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]\n        action: replace\n        target_label: __metrics_path__\n        regex: (.+)\n      - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]\n        action: replace\n        target_label: __address__\n        regex: ([^:]+)(?::\\d+)?;(\\d+)\n        replacement: $1:$2\n      - action: labelmap\n        regex: __meta_kubernetes_service_label_(.+)\n      - source_labels: [__meta_kubernetes_namespace]\n        action: replace\n        target_label: kubernetes_namespace\n      - source_labels: [__meta_kubernetes_service_name]\n        action: replace\n        target_label: kubernetes_name\n    - job_name: 'kubernetes-services'\n      metrics_path: /probe\n      params:\n        module: [http_2xx]\n      kubernetes_sd_configs:\n      - role: service\n      relabel_configs:\n      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]\n        action: keep\n        regex: true\n      - source_labels: [__address__]\n        target_label: __param_target\n      - target_label: __address__\n        replacement: blackbox-exporter.example.com:9115\n      - source_labels: [__param_target]\n        target_label: instance\n      - action: labelmap\n        regex: __meta_kubernetes_service_label_(.+)\n      - source_labels: [__meta_kubernetes_namespace]\n        target_label: kubernetes_namespace\n      - source_labels: [__meta_kubernetes_service_name]\n        target_label: kubernetes_name\n    - job_name: 'kubernetes-pods'\n      kubernetes_sd_configs:\n      - role: pod\n      relabel_configs:\n      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]\n        action: keep\n        regex: true\n      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]\n        action: replace\n        target_label: __metrics_path__\n        regex: (.+)\n      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]\n        action: replace\n        regex: ([^:]+)(?::\\d+)?;(\\d+)\n        replacement: $1:$2\n        target_label: __address__\n      - action: labelmap\n        regex: __meta_kubernetes_pod_label_(.+)\n      - source_labels: [__meta_kubernetes_namespace]\n        action: replace\n        target_label: kubernetes_namespace\n      - source_labels: [__meta_kubernetes_pod_name]\n        action: replace\n        target_label: kubernetes_pod_name\n    - job_name: 'kubernetes-ingresses'\n      kubernetes_sd_configs:\n      - role: ingress\n      relabel_configs:\n      - source_labels: [__meta_kubernetes_ingress_annotation_prometheus_io_probe]\n        action: keep\n        regex: true\n      - source_labels: [__meta_kubernetes_ingress_scheme,__address__,__meta_kubernetes_ingress_path]\n        regex: (.+);(.+);(.+)\n        replacement: 1://23\n        target_label: __param_target\n      - target_label: __address__\n        replacement: blackbox-exporter.example.com:9115\n      - source_labels: [__param_target]\n        target_label: instance\n      - action: labelmap\n        regex: __meta_kubernetes_ingress_label_(.+)\n      - source_labels: [__meta_kubernetes_namespace]\n        target_label: kubernetes_namespace\n      - source_labels: [__meta_kubernetes_ingress_name]\n        target_label: kubernetes_name\n    - job_name: 'kube-state-metrics'\n      static_configs:\n        - targets: ['kube-state-metrics.monitoring.svc.cluster.local:8080']\n    - job_name: 'node-exporter'\n      kubernetes_sd_configs:\n      - role: endpoints\n      relabel_configs:\n      - source_labels: [__meta_kubernetes_endpoints_name]\n        regex: 'node-exporter'\n        action: keep\nkind: ConfigMap\nmetadata:\n  name: prometheus-configmap\n  namespace: monitoring\n",
              "/api/v1/namespaces/monitoring/serviceaccounts/kube-state-metrics": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: 2.1.0\n  name: kube-state-metrics\n  namespace: monitoring\n",
              "/api/v1/namespaces/monitoring/serviceaccounts/node-exporter": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    app.kubernetes.io/component: exporter\n    app.kubernetes.io/name: node-exporter\n  name: node-exporter\n  namespace: monitoring\n",
              "/api/v1/namespaces/monitoring/serviceaccounts/prometheus": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    app: prometheus\n  name: prometheus\n  namespace: monitoring\n",
              "/api/v1/namespaces/monitoring/services/grafana": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: grafana\n    component: core\n  name: grafana\n  namespace: monitoring\nspec:\n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 3000\n  selector:\n    app: grafana\n    component: core\n  type: LoadBalancer\n",
              "/api/v1/namespaces/monitoring/services/kube-state-metrics": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: 2.1.0\n  name: kube-state-metrics\n  namespace: monitoring\nspec:\n  clusterIP: None\n  ports:\n  - name: http-metrics\n    port: 8080\n    targetPort: http-metrics\n  - name: telemetry\n    port: 8081\n    targetPort: telemetry\n  selector:\n    app.kubernetes.io/name: kube-state-metrics\n",
              "/api/v1/namespaces/monitoring/services/node-exporter": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/component: exporter\n    app.kubernetes.io/name: node-exporter\n  name: node-exporter\n  namespace: monitoring\nspec:\n  clusterIP: None\n  ports:\n  - name: http\n    port: 9100\n    targetPort: 9100\n  selector:\n    app.kubernetes.io/component: exporter\n    app.kubernetes.io/name: node-exporter\n",
              "/api/v1/namespaces/monitoring/services/prometheus": "apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    prometheus.io/scrape: \"true\"\n  labels:\n    name: prometheus\n  name: prometheus\n  namespace: monitoring\nspec:\n  ports:\n  - name: prometheus\n    port: 9090\n    protocol: TCP\n    targetPort: 9090\n  selector:\n    app: prometheus\n  type: LoadBalancer\n",
              "/apis/apps/v1/namespaces/monitoring/daemonsets/node-directory-size-metrics": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  annotations:\n    description: |\n      This `DaemonSet` provides metrics in Prometheus format about disk usage on the nodes.\n      The container `read-du` reads in sizes of all directories below /mnt and writes that to `/tmp/metrics`. It only reports directories larger then `100M` for now.\n      The other container `caddy` just hands out the contents of that file on request via `http` on `/metrics` at port `9102` which are the defaults for Prometheus.\n      These are scheduled on every node in the Kubernetes cluster.\n      To choose directories from the node to check, just mount them on the `read-du` container below `/mnt`.\n  name: node-directory-size-metrics\n  namespace: monitoring\nspec:\n  selector:\n    matchLabels:\n      app: node-directory-size-metrics\n  template:\n    metadata:\n      annotations:\n        description: |\n          This `Pod` provides metrics in Prometheus format about disk usage on the node.\n          The container `read-du` reads in sizes of all directories below /mnt and writes that to `/tmp/metrics`. It only reports directories larger then `100M` for now.\n          The other container `caddy` just hands out the contents of that file on request on `/metrics` at port `9102` which are the defaults for Prometheus.\n          This `Pod` is scheduled on every node in the Kubernetes cluster.\n          To choose directories from the node to check just mount them on `read-du` below `/mnt`.\n        prometheus.io/port: \"9102\"\n        prometheus.io/scrape: \"true\"\n      labels:\n        app: node-directory-size-metrics\n    spec:\n      containers:\n      - command:\n        - fish\n        - --command\n        - |\n          while true\n            for directory in (du --bytes --separate-dirs --threshold=100M /mnt)\n              echo $directory | read size path\n              echo \"node_directory_size_bytes{path=\\\"$path\\\"} $size\" \\\n                \u003e\u003e /tmp/metrics-temp\n            end\n            mv /tmp/metrics-temp /tmp/metrics\n            sleep 300\n          end\n        image: giantswarm/tiny-tools\n        imagePullPolicy: Always\n        name: read-du\n        volumeMounts:\n        - mountPath: /mnt/var\n          name: host-fs-var\n          readOnly: true\n        - mountPath: /tmp\n          name: metrics\n      - command:\n        - caddy\n        - -port=9102\n        - -root=/var/www\n        image: dockermuenster/caddy:0.9.3\n        name: caddy\n        ports:\n        - containerPort: 9102\n        volumeMounts:\n        - mountPath: /var/www\n          name: metrics\n      volumes:\n      - hostPath:\n          path: /var\n        name: host-fs-var\n      - emptyDir:\n          medium: Memory\n        name: metrics\n",
              "/apis/apps/v1/namespaces/monitoring/daemonsets/node-exporter": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/component: exporter\n    app.kubernetes.io/name: node-exporter\n  name: node-exporter\n  namespace: monitoring\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: exporter\n      app.kubernetes.io/name: node-exporter\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: exporter\n        app.kubernetes.io/name: node-exporter\n    spec:\n      containers:\n      - args:\n        - --web.listen-address=0.0.0.0:9100\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --no-collector.wifi\n        - --no-collector.hwmon\n        - --collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/pods/.+)($|/)\n        - --collector.netclass.ignored-devices=^(veth.*)$\n        - --collector.netdev.device-exclude=^(veth.*)$\n        image: quay.io/prometheus/node-exporter:v1.1.2\n        name: node-exporter\n        ports:\n        - containerPort: 9100\n          hostPort: 9100\n          name: http\n        resources:\n          limits:\n            cpu: 250m\n            memory: 180Mi\n          requests:\n            cpu: 102m\n            memory: 180Mi\n        volumeMounts:\n        - mountPath: /host/sys\n          mountPropagation: HostToContainer\n          name: sys\n          readOnly: true\n        - mountPath: /host/root\n          mountPropagation: HostToContainer\n          name: root\n          readOnly: true\n      hostNetwork: true\n      hostPID: true\n      nodeSelector:\n        kubernetes.io/os: linux\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: node-exporter\n      tolerations:\n      - operator: Exists\n      volumes:\n      - hostPath:\n          path: /sys\n        name: sys\n      - hostPath:\n          path: /\n        name: root\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 10%\n    type: RollingUpdate\n",
              "/apis/apps/v1/namespaces/monitoring/deployments/grafana-core": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: grafana\n    component: core\n  name: grafana-core\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: grafana\n      component: core\n  template:\n    metadata:\n      labels:\n        app: grafana\n        component: core\n    spec:\n      containers:\n      - env:\n        - name: GF_SECURITY_ADMIN_USER\n          value: admin\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          value: admin\n        image: grafana/grafana:7.5.5\n        imagePullPolicy: IfNotPresent\n        name: grafana-core\n        readinessProbe:\n          httpGet:\n            path: /login\n            port: 3000\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        volumeMounts:\n        - mountPath: /var/lib/grafana\n          name: grafana-persistent-storage\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n      volumes:\n      - emptyDir: {}\n        name: grafana-persistent-storage\n",
              "/apis/apps/v1/namespaces/monitoring/deployments/kube-state-metrics": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: 2.1.0\n  name: kube-state-metrics\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: 2.1.0\n    spec:\n      containers:\n      - image: k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.1.0\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        name: kube-state-metrics\n        ports:\n        - containerPort: 8080\n          name: http-metrics\n        - containerPort: 8081\n          name: telemetry\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        securityContext:\n          runAsUser: 65534\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: kube-state-metrics\n",
              "/apis/apps/v1/namespaces/monitoring/deployments/prometheus-deployment": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus-deployment\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus\n  strategy:\n    rollingUpdate:\n      maxSurge: 0\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: prometheus\n      name: prometheus\n    spec:\n      containers:\n      - args:\n        - --storage.tsdb.retention=360h\n        - --config.file=/etc/prometheus/prometheus.yml\n        image: prom/prometheus:v2.26.0\n        name: prometheus\n        ports:\n        - containerPort: 9090\n          name: web\n        volumeMounts:\n        - mountPath: /etc/prometheus\n          name: config-volume\n        - mountPath: /etc/prometheus-rules\n          name: alertrules-volume\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n      serviceAccount: prometheus\n      volumes:\n      - configMap:\n          name: prometheus-configmap\n        name: config-volume\n      - configMap:\n          name: prometheus-alertrules\n        name: alertrules-volume\n",
              "/apis/batch/v1/namespaces/monitoring/jobs/grafana-import-dashboards": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: grafana\n    component: import-dashboards\n  name: grafana-import-dashboards\n  namespace: monitoring\nspec:\n  template:\n    metadata:\n      annotations:\n        pod.beta.kubernetes.io/init-containers: '[ { \"name\": \"wait-for-endpoints\",\n          \"image\": \"giantswarm/tiny-tools\", \"imagePullPolicy\": \"IfNotPresent\", \"command\":\n          [\"fish\", \"-c\", \"echo \\\"waiting for endpoints...\\\"; while true; set endpoints\n          (curl -s --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt --header\n          \\\"Authorization: Bearer \\\"(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\n          https://kubernetes.default.svc/api/v1/namespaces/monitoring/endpoints/grafana);\n          echo $endpoints | jq \\\".\\\"; if test (echo $endpoints | jq -r \\\".subsets[].addresses\n          | length\\\") -gt 0; exit 0; end; echo \\\"waiting...\\\";sleep 1; end\"], \"args\":\n          [\"monitoring\", \"grafana\"] } ]'\n      labels:\n        app: grafana\n        component: import-dashboards\n      name: grafana-import-dashboards\n    spec:\n      containers:\n      - args:\n        - |\n          for file in *-datasource.json ; do\n            if [ -e \"$file\" ] ; then\n              echo \"importing $file\" \u0026\u0026\n              curl --silent --fail --show-error \\\n                --request POST http://admin:admin@grafana/api/datasources \\\n                --header \"Content-Type: application/json\" \\\n                --header \"Accept: application/json\" \\\n                --data-binary \"@$file\" ;\n              echo \"\" ;\n            fi\n          done ; for file in *-dashboard.json ; do\n            if [ -e \"$file\" ] ; then\n              echo \"importing $file\" \u0026\u0026\n              curl --silent --fail --show-error \\\n                --request POST http://admin:admin@grafana/api/dashboards/import \\\n                --header \"Content-Type: application/json\" \\\n                --header \"Accept: application/json\" \\\n                --data-binary \"@$file\" ;\n              echo \"\" ;\n            fi\n          done ;\n        command:\n        - /bin/sh\n        - -c\n        image: giantswarm/tiny-tools\n        name: grafana-import-dashboards\n        volumeMounts:\n        - mountPath: /opt/grafana-import-dashboards\n          name: config-volume\n        workingDir: /opt/grafana-import-dashboards\n      restartPolicy: Never\n      volumes:\n      - configMap:\n          name: grafana-import-dashboards\n        name: config-volume\n",
              "/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/kube-state-metrics": "apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: 2.1.0\n  name: kube-state-metrics\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: kube-state-metrics\nsubjects:\n- kind: ServiceAccount\n  name: kube-state-metrics\n  namespace: monitoring\n",
              "/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/prometheus": "apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  labels:\n    app: prometheus\n  name: prometheus\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: prometheus\nsubjects:\n- kind: ServiceAccount\n  name: prometheus\n  namespace: monitoring\n",
              "/apis/rbac.authorization.k8s.io/v1/clusterroles/kube-state-metrics": "apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: 2.1.0\n  name: kube-state-metrics\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - configmaps\n  - secrets\n  - nodes\n  - pods\n  - services\n  - resourcequotas\n  - replicationcontrollers\n  - limitranges\n  - persistentvolumeclaims\n  - persistentvolumes\n  - namespaces\n  - endpoints\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - apps\n  resources:\n  - statefulsets\n  - daemonsets\n  - deployments\n  - replicasets\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - batch\n  resources:\n  - cronjobs\n  - jobs\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - autoscaling\n  resources:\n  - horizontalpodautoscalers\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - authentication.k8s.io\n  resources:\n  - tokenreviews\n  verbs:\n  - create\n- apiGroups:\n  - authorization.k8s.io\n  resources:\n  - subjectaccessreviews\n  verbs:\n  - create\n- apiGroups:\n  - policy\n  resources:\n  - poddisruptionbudgets\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - certificates.k8s.io\n  resources:\n  - certificatesigningrequests\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - storage.k8s.io\n  resources:\n  - storageclasses\n  - volumeattachments\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - admissionregistration.k8s.io\n  resources:\n  - mutatingwebhookconfigurations\n  - validatingwebhookconfigurations\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - networkpolicies\n  - ingresses\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - coordination.k8s.io\n  resources:\n  - leases\n  verbs:\n  - list\n  - watch\n",
              "/apis/rbac.authorization.k8s.io/v1/clusterroles/prometheus": "apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  labels:\n    app: prometheus\n  name: prometheus\nrules:\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - ingresses\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - \"\"\n  resources:\n  - nodes\n  - nodes/metrics\n  - nodes/proxy\n  - services\n  - endpoints\n  - pods\n  verbs:\n  - get\n  - list\n  - watch\n- nonResourceURLs:\n  - /metrics\n  verbs:\n  - get\n"
            },
            "pattern": "./*.yaml",
            "sensitive_vars": null,
            "vars": null
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubectl_manifest",
      "name": "monitoring",
      "provider": "provider[\"registry.terraform.io/gavinbunney/kubectl\"]",
      "instances": [
        {
          "index_key": "# Useful examples on how to configure Prometheus\n# * https://www.weave.works/prometheus-and-kubernetes-monitoring-your-applications/\n# * https://grafana.net/dashboards/162\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: prometheus-alertrules\n  namespace: monitoring\ndata:\n  alert.rules: |-\n    groups:\n    - name: HighErrorRate\n      rules:\n      - alert: HighErrorRate\n        expr: rate(request_duration_seconds_count{status_code=\"500\"}[5m]) \u003e 1\n        for: 5m\n        labels:\n          severity: slack\n        annotations:\n          summary: \"High HTTP 500 error rates\"\n          description: \"Rate of HTTP 500 errors per 5 minutes: {{ $value }}\"",
          "schema_version": 1,
          "attributes": {
            "api_version": "v1",
            "apply_only": false,
            "force_conflicts": false,
            "force_new": false,
            "id": "/api/v1/namespaces/monitoring/configmaps/prometheus-alertrules",
            "ignore_fields": null,
            "kind": "ConfigMap",
            "live_manifest_incluster": "c81435fe527281323cc72c31a90eb72ae5ecb96c3e18b0e38369c233d8499c00",
            "live_uid": "fa207975-b7f3-4fcb-a13e-de1332fd65fa",
            "name": "prometheus-alertrules",
            "namespace": "monitoring",
            "override_namespace": null,
            "sensitive_fields": null,
            "server_side_apply": false,
            "timeouts": null,
            "uid": "fa207975-b7f3-4fcb-a13e-de1332fd65fa",
            "validate_schema": true,
            "wait": null,
            "wait_for_rollout": true,
            "yaml_body": "# Useful examples on how to configure Prometheus\n# * https://www.weave.works/prometheus-and-kubernetes-monitoring-your-applications/\n# * https://grafana.net/dashboards/162\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: prometheus-alertrules\n  namespace: monitoring\ndata:\n  alert.rules: |-\n    groups:\n    - name: HighErrorRate\n      rules:\n      - alert: HighErrorRate\n        expr: rate(request_duration_seconds_count{status_code=\"500\"}[5m]) \u003e 1\n        for: 5m\n        labels:\n          severity: slack\n        annotations:\n          summary: \"High HTTP 500 error rates\"\n          description: \"Rate of HTTP 500 errors per 5 minutes: {{ $value }}\"",
            "yaml_body_parsed": "apiVersion: v1\ndata:\n  alert.rules: |-\n    groups:\n    - name: HighErrorRate\n      rules:\n      - alert: HighErrorRate\n        expr: rate(request_duration_seconds_count{status_code=\"500\"}[5m]) \u003e 1\n        for: 5m\n        labels:\n          severity: slack\n        annotations:\n          summary: \"High HTTP 500 error rates\"\n          description: \"Rate of HTTP 500 errors per 5 minutes: {{ $value }}\"\nkind: ConfigMap\nmetadata:\n  name: prometheus-alertrules\n  namespace: monitoring\n",
            "yaml_incluster": "c81435fe527281323cc72c31a90eb72ae5ecb96c3e18b0e38369c233d8499c00"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDB9LCJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "data.aws_eks_cluster.eks-cluster",
            "data.kubectl_path_documents.docs"
          ]
        },
        {
          "index_key": "---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: prometheus\n  labels:\n    app: prometheus\nrules:\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - ingresses\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups: [\"\"] # \"\" indicates the core API group\n  resources:\n  - nodes\n  - nodes/metrics\n  - nodes/proxy\n  - services\n  - endpoints\n  - pods\n  verbs:\n  - get\n  - list\n  - watch\n- nonResourceURLs:\n  - /metrics\n  verbs:\n  - get",
          "schema_version": 1,
          "attributes": {
            "api_version": "rbac.authorization.k8s.io/v1",
            "apply_only": false,
            "force_conflicts": false,
            "force_new": false,
            "id": "/apis/rbac.authorization.k8s.io/v1/clusterroles/prometheus",
            "ignore_fields": null,
            "kind": "ClusterRole",
            "live_manifest_incluster": "8f1053ce6b05e0eb7e2024aca8ecee88a40dadd70e165b662c12c870895fa588",
            "live_uid": "d8b1cc2e-3b31-4e41-90db-aeeff0d6bad3",
            "name": "prometheus",
            "namespace": null,
            "override_namespace": null,
            "sensitive_fields": null,
            "server_side_apply": false,
            "timeouts": null,
            "uid": "d8b1cc2e-3b31-4e41-90db-aeeff0d6bad3",
            "validate_schema": true,
            "wait": null,
            "wait_for_rollout": true,
            "yaml_body": "---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: prometheus\n  labels:\n    app: prometheus\nrules:\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - ingresses\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups: [\"\"] # \"\" indicates the core API group\n  resources:\n  - nodes\n  - nodes/metrics\n  - nodes/proxy\n  - services\n  - endpoints\n  - pods\n  verbs:\n  - get\n  - list\n  - watch\n- nonResourceURLs:\n  - /metrics\n  verbs:\n  - get",
            "yaml_body_parsed": "apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  labels:\n    app: prometheus\n  name: prometheus\nrules:\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - ingresses\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - \"\"\n  resources:\n  - nodes\n  - nodes/metrics\n  - nodes/proxy\n  - services\n  - endpoints\n  - pods\n  verbs:\n  - get\n  - list\n  - watch\n- nonResourceURLs:\n  - /metrics\n  verbs:\n  - get\n",
            "yaml_incluster": "8f1053ce6b05e0eb7e2024aca8ecee88a40dadd70e165b662c12c870895fa588"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDB9LCJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "data.aws_eks_cluster.eks-cluster",
            "data.kubectl_path_documents.docs"
          ]
        },
        {
          "index_key": "---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: prometheus\n  labels:\n    app: prometheus\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: prometheus\nsubjects:\n- kind: ServiceAccount\n  name: prometheus\n  namespace: monitoring",
          "schema_version": 1,
          "attributes": {
            "api_version": "rbac.authorization.k8s.io/v1",
            "apply_only": false,
            "force_conflicts": false,
            "force_new": false,
            "id": "/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/prometheus",
            "ignore_fields": null,
            "kind": "ClusterRoleBinding",
            "live_manifest_incluster": "59e8f783a3bb9cf2f9d5de0e34f7dc092a7c465374c0ce968dfa3de8bf26780a",
            "live_uid": "01070eab-0a1e-4b02-8f23-fedef9d01303",
            "name": "prometheus",
            "namespace": null,
            "override_namespace": null,
            "sensitive_fields": null,
            "server_side_apply": false,
            "timeouts": null,
            "uid": "01070eab-0a1e-4b02-8f23-fedef9d01303",
            "validate_schema": true,
            "wait": null,
            "wait_for_rollout": true,
            "yaml_body": "---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: prometheus\n  labels:\n    app: prometheus\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: prometheus\nsubjects:\n- kind: ServiceAccount\n  name: prometheus\n  namespace: monitoring",
            "yaml_body_parsed": "apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  labels:\n    app: prometheus\n  name: prometheus\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: prometheus\nsubjects:\n- kind: ServiceAccount\n  name: prometheus\n  namespace: monitoring\n",
            "yaml_incluster": "59e8f783a3bb9cf2f9d5de0e34f7dc092a7c465374c0ce968dfa3de8bf26780a"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDB9LCJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "data.aws_eks_cluster.eks-cluster",
            "data.kubectl_path_documents.docs"
          ]
        },
        {
          "index_key": "---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: prometheus\n  namespace: monitoring\n  labels:\n    app: prometheus",
          "schema_version": 1,
          "attributes": {
            "api_version": "v1",
            "apply_only": false,
            "force_conflicts": false,
            "force_new": false,
            "id": "/api/v1/namespaces/monitoring/serviceaccounts/prometheus",
            "ignore_fields": null,
            "kind": "ServiceAccount",
            "live_manifest_incluster": "66680583c20cd08e9cc1a93ae96fdcf3b0c30755b755f243ba315fab75f68bdc",
            "live_uid": "91eeca18-0f97-45d6-be7a-be6c9a940038",
            "name": "prometheus",
            "namespace": "monitoring",
            "override_namespace": null,
            "sensitive_fields": null,
            "server_side_apply": false,
            "timeouts": null,
            "uid": "91eeca18-0f97-45d6-be7a-be6c9a940038",
            "validate_schema": true,
            "wait": null,
            "wait_for_rollout": true,
            "yaml_body": "---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: prometheus\n  namespace: monitoring\n  labels:\n    app: prometheus",
            "yaml_body_parsed": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    app: prometheus\n  name: prometheus\n  namespace: monitoring\n",
            "yaml_incluster": "66680583c20cd08e9cc1a93ae96fdcf3b0c30755b755f243ba315fab75f68bdc"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDB9LCJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "data.aws_eks_cluster.eks-cluster",
            "data.kubectl_path_documents.docs"
          ]
        },
        {
          "index_key": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/component: exporter\n    app.kubernetes.io/name: node-exporter\n  name: node-exporter\n  namespace: monitoring\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: exporter\n      app.kubernetes.io/name: node-exporter\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: exporter\n        app.kubernetes.io/name: node-exporter\n    spec:\n      containers:\n      - args:\n        - --web.listen-address=0.0.0.0:9100\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --no-collector.wifi\n        - --no-collector.hwmon\n        - --collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/pods/.+)($|/)\n        - --collector.netclass.ignored-devices=^(veth.*)$\n        - --collector.netdev.device-exclude=^(veth.*)$\n        image: quay.io/prometheus/node-exporter:v1.1.2\n        name: node-exporter\n        resources:\n          limits:\n            cpu: 250m\n            memory: 180Mi\n          requests:\n            cpu: 102m\n            memory: 180Mi\n        volumeMounts:\n        - mountPath: /host/sys\n          mountPropagation: HostToContainer\n          name: sys\n          readOnly: true\n        - mountPath: /host/root\n          mountPropagation: HostToContainer\n          name: root\n          readOnly: true\n        ports:\n        - containerPort: 9100\n          hostPort: 9100\n          name: http\n      hostNetwork: true\n      hostPID: true\n      nodeSelector:\n        kubernetes.io/os: linux\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: node-exporter\n      tolerations:\n      - operator: Exists\n      volumes:\n      - hostPath:\n          path: /sys\n        name: sys\n      - hostPath:\n          path: /\n        name: root\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 10%\n    type: RollingUpdate",
          "schema_version": 1,
          "attributes": {
            "api_version": "apps/v1",
            "apply_only": false,
            "force_conflicts": false,
            "force_new": false,
            "id": "/apis/apps/v1/namespaces/monitoring/daemonsets/node-exporter",
            "ignore_fields": null,
            "kind": "DaemonSet",
            "live_manifest_incluster": "acc6ea057ae96dd2ba2156733357e80210964844e160f44dd79acb9d8c21cc2b",
            "live_uid": "89d765a2-542c-4533-8876-b4d216f81673",
            "name": "node-exporter",
            "namespace": "monitoring",
            "override_namespace": null,
            "sensitive_fields": null,
            "server_side_apply": false,
            "timeouts": null,
            "uid": "89d765a2-542c-4533-8876-b4d216f81673",
            "validate_schema": true,
            "wait": null,
            "wait_for_rollout": true,
            "yaml_body": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/component: exporter\n    app.kubernetes.io/name: node-exporter\n  name: node-exporter\n  namespace: monitoring\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: exporter\n      app.kubernetes.io/name: node-exporter\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: exporter\n        app.kubernetes.io/name: node-exporter\n    spec:\n      containers:\n      - args:\n        - --web.listen-address=0.0.0.0:9100\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --no-collector.wifi\n        - --no-collector.hwmon\n        - --collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/pods/.+)($|/)\n        - --collector.netclass.ignored-devices=^(veth.*)$\n        - --collector.netdev.device-exclude=^(veth.*)$\n        image: quay.io/prometheus/node-exporter:v1.1.2\n        name: node-exporter\n        resources:\n          limits:\n            cpu: 250m\n            memory: 180Mi\n          requests:\n            cpu: 102m\n            memory: 180Mi\n        volumeMounts:\n        - mountPath: /host/sys\n          mountPropagation: HostToContainer\n          name: sys\n          readOnly: true\n        - mountPath: /host/root\n          mountPropagation: HostToContainer\n          name: root\n          readOnly: true\n        ports:\n        - containerPort: 9100\n          hostPort: 9100\n          name: http\n      hostNetwork: true\n      hostPID: true\n      nodeSelector:\n        kubernetes.io/os: linux\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: node-exporter\n      tolerations:\n      - operator: Exists\n      volumes:\n      - hostPath:\n          path: /sys\n        name: sys\n      - hostPath:\n          path: /\n        name: root\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 10%\n    type: RollingUpdate",
            "yaml_body_parsed": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/component: exporter\n    app.kubernetes.io/name: node-exporter\n  name: node-exporter\n  namespace: monitoring\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: exporter\n      app.kubernetes.io/name: node-exporter\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: exporter\n        app.kubernetes.io/name: node-exporter\n    spec:\n      containers:\n      - args:\n        - --web.listen-address=0.0.0.0:9100\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --no-collector.wifi\n        - --no-collector.hwmon\n        - --collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/pods/.+)($|/)\n        - --collector.netclass.ignored-devices=^(veth.*)$\n        - --collector.netdev.device-exclude=^(veth.*)$\n        image: quay.io/prometheus/node-exporter:v1.1.2\n        name: node-exporter\n        ports:\n        - containerPort: 9100\n          hostPort: 9100\n          name: http\n        resources:\n          limits:\n            cpu: 250m\n            memory: 180Mi\n          requests:\n            cpu: 102m\n            memory: 180Mi\n        volumeMounts:\n        - mountPath: /host/sys\n          mountPropagation: HostToContainer\n          name: sys\n          readOnly: true\n        - mountPath: /host/root\n          mountPropagation: HostToContainer\n          name: root\n          readOnly: true\n      hostNetwork: true\n      hostPID: true\n      nodeSelector:\n        kubernetes.io/os: linux\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: node-exporter\n      tolerations:\n      - operator: Exists\n      volumes:\n      - hostPath:\n          path: /sys\n        name: sys\n      - hostPath:\n          path: /\n        name: root\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 10%\n    type: RollingUpdate\n",
            "yaml_incluster": "acc6ea057ae96dd2ba2156733357e80210964844e160f44dd79acb9d8c21cc2b"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDB9LCJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "data.aws_eks_cluster.eks-cluster",
            "data.kubectl_path_documents.docs"
          ]
        },
        {
          "index_key": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-directory-size-metrics\n  namespace: monitoring\n  annotations:\n    description: |\n      This `DaemonSet` provides metrics in Prometheus format about disk usage on the nodes.\n      The container `read-du` reads in sizes of all directories below /mnt and writes that to `/tmp/metrics`. It only reports directories larger then `100M` for now.\n      The other container `caddy` just hands out the contents of that file on request via `http` on `/metrics` at port `9102` which are the defaults for Prometheus.\n      These are scheduled on every node in the Kubernetes cluster.\n      To choose directories from the node to check, just mount them on the `read-du` container below `/mnt`.\nspec:\n  selector:\n    matchLabels:\n      app: node-directory-size-metrics\n  template:\n    metadata:\n      labels:\n        app: node-directory-size-metrics\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9102'\n        description: |\n          This `Pod` provides metrics in Prometheus format about disk usage on the node.\n          The container `read-du` reads in sizes of all directories below /mnt and writes that to `/tmp/metrics`. It only reports directories larger then `100M` for now.\n          The other container `caddy` just hands out the contents of that file on request on `/metrics` at port `9102` which are the defaults for Prometheus.\n          This `Pod` is scheduled on every node in the Kubernetes cluster.\n          To choose directories from the node to check just mount them on `read-du` below `/mnt`.\n    spec:\n      containers:\n      - name: read-du\n        image: giantswarm/tiny-tools\n        imagePullPolicy: Always\n        # FIXME threshold via env var\n        # The\n        command:\n        - fish\n        - --command\n        - |\n          while true\n            for directory in (du --bytes --separate-dirs --threshold=100M /mnt)\n              echo $directory | read size path\n              echo \"node_directory_size_bytes{path=\\\"$path\\\"} $size\" \\\n                \u003e\u003e /tmp/metrics-temp\n            end\n            mv /tmp/metrics-temp /tmp/metrics\n            sleep 300\n          end\n        volumeMounts:\n        - name: host-fs-var\n          mountPath: /mnt/var\n          readOnly: true\n        - name: metrics\n          mountPath: /tmp\n      - name: caddy\n        image: dockermuenster/caddy:0.9.3\n        command:\n        - \"caddy\"\n        - \"-port=9102\"\n        - \"-root=/var/www\"\n        ports:\n        - containerPort: 9102\n        volumeMounts:\n        - name: metrics\n          mountPath: /var/www\n      volumes:\n      - name: host-fs-var\n        hostPath:\n          path: /var\n      - name: metrics\n        emptyDir:\n          medium: Memory",
          "schema_version": 1,
          "attributes": {
            "api_version": "apps/v1",
            "apply_only": false,
            "force_conflicts": false,
            "force_new": false,
            "id": "/apis/apps/v1/namespaces/monitoring/daemonsets/node-directory-size-metrics",
            "ignore_fields": null,
            "kind": "DaemonSet",
            "live_manifest_incluster": "a3931fc8e61d2f042e8c21ba08ab5566ff3ad2dc53401b50e9024dfe59abaff7",
            "live_uid": "e078cb2b-5b25-493a-b8bf-896f480dfdf4",
            "name": "node-directory-size-metrics",
            "namespace": "monitoring",
            "override_namespace": null,
            "sensitive_fields": null,
            "server_side_apply": false,
            "timeouts": null,
            "uid": "e078cb2b-5b25-493a-b8bf-896f480dfdf4",
            "validate_schema": true,
            "wait": null,
            "wait_for_rollout": true,
            "yaml_body": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-directory-size-metrics\n  namespace: monitoring\n  annotations:\n    description: |\n      This `DaemonSet` provides metrics in Prometheus format about disk usage on the nodes.\n      The container `read-du` reads in sizes of all directories below /mnt and writes that to `/tmp/metrics`. It only reports directories larger then `100M` for now.\n      The other container `caddy` just hands out the contents of that file on request via `http` on `/metrics` at port `9102` which are the defaults for Prometheus.\n      These are scheduled on every node in the Kubernetes cluster.\n      To choose directories from the node to check, just mount them on the `read-du` container below `/mnt`.\nspec:\n  selector:\n    matchLabels:\n      app: node-directory-size-metrics\n  template:\n    metadata:\n      labels:\n        app: node-directory-size-metrics\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9102'\n        description: |\n          This `Pod` provides metrics in Prometheus format about disk usage on the node.\n          The container `read-du` reads in sizes of all directories below /mnt and writes that to `/tmp/metrics`. It only reports directories larger then `100M` for now.\n          The other container `caddy` just hands out the contents of that file on request on `/metrics` at port `9102` which are the defaults for Prometheus.\n          This `Pod` is scheduled on every node in the Kubernetes cluster.\n          To choose directories from the node to check just mount them on `read-du` below `/mnt`.\n    spec:\n      containers:\n      - name: read-du\n        image: giantswarm/tiny-tools\n        imagePullPolicy: Always\n        # FIXME threshold via env var\n        # The\n        command:\n        - fish\n        - --command\n        - |\n          while true\n            for directory in (du --bytes --separate-dirs --threshold=100M /mnt)\n              echo $directory | read size path\n              echo \"node_directory_size_bytes{path=\\\"$path\\\"} $size\" \\\n                \u003e\u003e /tmp/metrics-temp\n            end\n            mv /tmp/metrics-temp /tmp/metrics\n            sleep 300\n          end\n        volumeMounts:\n        - name: host-fs-var\n          mountPath: /mnt/var\n          readOnly: true\n        - name: metrics\n          mountPath: /tmp\n      - name: caddy\n        image: dockermuenster/caddy:0.9.3\n        command:\n        - \"caddy\"\n        - \"-port=9102\"\n        - \"-root=/var/www\"\n        ports:\n        - containerPort: 9102\n        volumeMounts:\n        - name: metrics\n          mountPath: /var/www\n      volumes:\n      - name: host-fs-var\n        hostPath:\n          path: /var\n      - name: metrics\n        emptyDir:\n          medium: Memory",
            "yaml_body_parsed": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  annotations:\n    description: |\n      This `DaemonSet` provides metrics in Prometheus format about disk usage on the nodes.\n      The container `read-du` reads in sizes of all directories below /mnt and writes that to `/tmp/metrics`. It only reports directories larger then `100M` for now.\n      The other container `caddy` just hands out the contents of that file on request via `http` on `/metrics` at port `9102` which are the defaults for Prometheus.\n      These are scheduled on every node in the Kubernetes cluster.\n      To choose directories from the node to check, just mount them on the `read-du` container below `/mnt`.\n  name: node-directory-size-metrics\n  namespace: monitoring\nspec:\n  selector:\n    matchLabels:\n      app: node-directory-size-metrics\n  template:\n    metadata:\n      annotations:\n        description: |\n          This `Pod` provides metrics in Prometheus format about disk usage on the node.\n          The container `read-du` reads in sizes of all directories below /mnt and writes that to `/tmp/metrics`. It only reports directories larger then `100M` for now.\n          The other container `caddy` just hands out the contents of that file on request on `/metrics` at port `9102` which are the defaults for Prometheus.\n          This `Pod` is scheduled on every node in the Kubernetes cluster.\n          To choose directories from the node to check just mount them on `read-du` below `/mnt`.\n        prometheus.io/port: \"9102\"\n        prometheus.io/scrape: \"true\"\n      labels:\n        app: node-directory-size-metrics\n    spec:\n      containers:\n      - command:\n        - fish\n        - --command\n        - |\n          while true\n            for directory in (du --bytes --separate-dirs --threshold=100M /mnt)\n              echo $directory | read size path\n              echo \"node_directory_size_bytes{path=\\\"$path\\\"} $size\" \\\n                \u003e\u003e /tmp/metrics-temp\n            end\n            mv /tmp/metrics-temp /tmp/metrics\n            sleep 300\n          end\n        image: giantswarm/tiny-tools\n        imagePullPolicy: Always\n        name: read-du\n        volumeMounts:\n        - mountPath: /mnt/var\n          name: host-fs-var\n          readOnly: true\n        - mountPath: /tmp\n          name: metrics\n      - command:\n        - caddy\n        - -port=9102\n        - -root=/var/www\n        image: dockermuenster/caddy:0.9.3\n        name: caddy\n        ports:\n        - containerPort: 9102\n        volumeMounts:\n        - mountPath: /var/www\n          name: metrics\n      volumes:\n      - hostPath:\n          path: /var\n        name: host-fs-var\n      - emptyDir:\n          medium: Memory\n        name: metrics\n",
            "yaml_incluster": "a3931fc8e61d2f042e8c21ba08ab5566ff3ad2dc53401b50e9024dfe59abaff7"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDB9LCJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "data.aws_eks_cluster.eks-cluster",
            "data.kubectl_path_documents.docs"
          ]
        },
        {
          "index_key": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: 2.1.0\n  name: kube-state-metrics\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: 2.1.0\n    spec:\n      containers:\n      - image: k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.1.0\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        name: kube-state-metrics\n        ports:\n        - containerPort: 8080\n          name: http-metrics\n        - containerPort: 8081\n          name: telemetry\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        securityContext:\n          runAsUser: 65534\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: kube-state-metrics",
          "schema_version": 1,
          "attributes": {
            "api_version": "apps/v1",
            "apply_only": false,
            "force_conflicts": false,
            "force_new": false,
            "id": "/apis/apps/v1/namespaces/monitoring/deployments/kube-state-metrics",
            "ignore_fields": null,
            "kind": "Deployment",
            "live_manifest_incluster": "4e6fecfcd51c7822f4cbbc064596e0cf30ef9667f85d1f4b9db9c061b4dd6dc0",
            "live_uid": "11bade94-cb05-4967-bd93-b6b397dfbbe8",
            "name": "kube-state-metrics",
            "namespace": "monitoring",
            "override_namespace": null,
            "sensitive_fields": null,
            "server_side_apply": false,
            "timeouts": null,
            "uid": "11bade94-cb05-4967-bd93-b6b397dfbbe8",
            "validate_schema": true,
            "wait": null,
            "wait_for_rollout": true,
            "yaml_body": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: 2.1.0\n  name: kube-state-metrics\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: 2.1.0\n    spec:\n      containers:\n      - image: k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.1.0\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        name: kube-state-metrics\n        ports:\n        - containerPort: 8080\n          name: http-metrics\n        - containerPort: 8081\n          name: telemetry\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        securityContext:\n          runAsUser: 65534\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: kube-state-metrics",
            "yaml_body_parsed": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: 2.1.0\n  name: kube-state-metrics\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: 2.1.0\n    spec:\n      containers:\n      - image: k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.1.0\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        name: kube-state-metrics\n        ports:\n        - containerPort: 8080\n          name: http-metrics\n        - containerPort: 8081\n          name: telemetry\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        securityContext:\n          runAsUser: 65534\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: kube-state-metrics\n",
            "yaml_incluster": "4e6fecfcd51c7822f4cbbc064596e0cf30ef9667f85d1f4b9db9c061b4dd6dc0"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDB9LCJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "data.aws_eks_cluster.eks-cluster",
            "data.kubectl_path_documents.docs"
          ]
        },
        {
          "index_key": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grafana-core\n  namespace: monitoring\n  labels:\n    app: grafana\n    component: core\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: grafana\n      component: core\n  template:\n    metadata:\n      labels:\n        app: grafana\n        component: core\n    spec:\n      containers:\n      - image: grafana/grafana:7.5.5\n        name: grafana-core\n        imagePullPolicy: IfNotPresent\n        # env:\n        resources:\n          # keep request = limit to keep this container in guaranteed class\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        env:\n          # The following env variables set up basic auth twith the default admin user and admin password.\n          - name: GF_SECURITY_ADMIN_USER\n            value: admin\n          - name: GF_SECURITY_ADMIN_PASSWORD\n            value: admin\n        readinessProbe:\n          httpGet:\n            path: /login\n            port: 3000\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: grafana-persistent-storage\n          mountPath: /var/lib/grafana\n      volumes:\n      - name: grafana-persistent-storage\n        emptyDir: {}\n      nodeSelector:\n        beta.kubernetes.io/os: linux",
          "schema_version": 1,
          "attributes": {
            "api_version": "apps/v1",
            "apply_only": false,
            "force_conflicts": false,
            "force_new": false,
            "id": "/apis/apps/v1/namespaces/monitoring/deployments/grafana-core",
            "ignore_fields": null,
            "kind": "Deployment",
            "live_manifest_incluster": "b5fee911004d0829783667cd13ded0c6cf613d9cec3965282ee5a167546ece7f",
            "live_uid": "acdb9a0c-2e55-45d9-b5d4-edfd1f070a9a",
            "name": "grafana-core",
            "namespace": "monitoring",
            "override_namespace": null,
            "sensitive_fields": null,
            "server_side_apply": false,
            "timeouts": null,
            "uid": "acdb9a0c-2e55-45d9-b5d4-edfd1f070a9a",
            "validate_schema": true,
            "wait": null,
            "wait_for_rollout": true,
            "yaml_body": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grafana-core\n  namespace: monitoring\n  labels:\n    app: grafana\n    component: core\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: grafana\n      component: core\n  template:\n    metadata:\n      labels:\n        app: grafana\n        component: core\n    spec:\n      containers:\n      - image: grafana/grafana:7.5.5\n        name: grafana-core\n        imagePullPolicy: IfNotPresent\n        # env:\n        resources:\n          # keep request = limit to keep this container in guaranteed class\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        env:\n          # The following env variables set up basic auth twith the default admin user and admin password.\n          - name: GF_SECURITY_ADMIN_USER\n            value: admin\n          - name: GF_SECURITY_ADMIN_PASSWORD\n            value: admin\n        readinessProbe:\n          httpGet:\n            path: /login\n            port: 3000\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: grafana-persistent-storage\n          mountPath: /var/lib/grafana\n      volumes:\n      - name: grafana-persistent-storage\n        emptyDir: {}\n      nodeSelector:\n        beta.kubernetes.io/os: linux",
            "yaml_body_parsed": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: grafana\n    component: core\n  name: grafana-core\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: grafana\n      component: core\n  template:\n    metadata:\n      labels:\n        app: grafana\n        component: core\n    spec:\n      containers:\n      - env:\n        - name: GF_SECURITY_ADMIN_USER\n          value: admin\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          value: admin\n        image: grafana/grafana:7.5.5\n        imagePullPolicy: IfNotPresent\n        name: grafana-core\n        readinessProbe:\n          httpGet:\n            path: /login\n            port: 3000\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        volumeMounts:\n        - mountPath: /var/lib/grafana\n          name: grafana-persistent-storage\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n      volumes:\n      - emptyDir: {}\n        name: grafana-persistent-storage\n",
            "yaml_incluster": "b5fee911004d0829783667cd13ded0c6cf613d9cec3965282ee5a167546ece7f"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDB9LCJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "data.aws_eks_cluster.eks-cluster",
            "data.kubectl_path_documents.docs"
          ]
        },
        {
          "index_key": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus-deployment\n  namespace: monitoring\nspec:\n  replicas: 1\n  strategy:\n    rollingUpdate:\n      maxSurge: 0\n      maxUnavailable: 1\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: prometheus\n  template:\n    metadata:\n      name: prometheus\n      labels:\n        app: prometheus\n    spec:\n      serviceAccount: prometheus\n      containers:\n      - name: prometheus\n        image: prom/prometheus:v2.26.0\n        args:\n        - '--storage.tsdb.retention=360h'\n        - '--config.file=/etc/prometheus/prometheus.yml'\n        ports:\n        - name: web\n          containerPort: 9090\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/prometheus\n        - name: alertrules-volume\n          mountPath: /etc/prometheus-rules\n      volumes:\n      - name: config-volume\n        configMap:\n          name: prometheus-configmap\n      - name: alertrules-volume\n        configMap:\n          name: prometheus-alertrules\n      nodeSelector:\n        beta.kubernetes.io/os: linux",
          "schema_version": 1,
          "attributes": {
            "api_version": "apps/v1",
            "apply_only": false,
            "force_conflicts": false,
            "force_new": false,
            "id": "/apis/apps/v1/namespaces/monitoring/deployments/prometheus-deployment",
            "ignore_fields": null,
            "kind": "Deployment",
            "live_manifest_incluster": "1d1acd850896172c058c372873ccbf6a98f7d32e9e544a6552b92123e4b0fb55",
            "live_uid": "42f5d0ed-3f3d-4dfb-b8b1-34fb1055f315",
            "name": "prometheus-deployment",
            "namespace": "monitoring",
            "override_namespace": null,
            "sensitive_fields": null,
            "server_side_apply": false,
            "timeouts": null,
            "uid": "42f5d0ed-3f3d-4dfb-b8b1-34fb1055f315",
            "validate_schema": true,
            "wait": null,
            "wait_for_rollout": true,
            "yaml_body": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus-deployment\n  namespace: monitoring\nspec:\n  replicas: 1\n  strategy:\n    rollingUpdate:\n      maxSurge: 0\n      maxUnavailable: 1\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: prometheus\n  template:\n    metadata:\n      name: prometheus\n      labels:\n        app: prometheus\n    spec:\n      serviceAccount: prometheus\n      containers:\n      - name: prometheus\n        image: prom/prometheus:v2.26.0\n        args:\n        - '--storage.tsdb.retention=360h'\n        - '--config.file=/etc/prometheus/prometheus.yml'\n        ports:\n        - name: web\n          containerPort: 9090\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/prometheus\n        - name: alertrules-volume\n          mountPath: /etc/prometheus-rules\n      volumes:\n      - name: config-volume\n        configMap:\n          name: prometheus-configmap\n      - name: alertrules-volume\n        configMap:\n          name: prometheus-alertrules\n      nodeSelector:\n        beta.kubernetes.io/os: linux",
            "yaml_body_parsed": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus-deployment\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus\n  strategy:\n    rollingUpdate:\n      maxSurge: 0\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: prometheus\n      name: prometheus\n    spec:\n      containers:\n      - args:\n        - --storage.tsdb.retention=360h\n        - --config.file=/etc/prometheus/prometheus.yml\n        image: prom/prometheus:v2.26.0\n        name: prometheus\n        ports:\n        - containerPort: 9090\n          name: web\n        volumeMounts:\n        - mountPath: /etc/prometheus\n          name: config-volume\n        - mountPath: /etc/prometheus-rules\n          name: alertrules-volume\n      nodeSelector:\n        beta.kubernetes.io/os: linux\n      serviceAccount: prometheus\n      volumes:\n      - configMap:\n          name: prometheus-configmap\n        name: config-volume\n      - configMap:\n          name: prometheus-alertrules\n        name: alertrules-volume\n",
            "yaml_incluster": "1d1acd850896172c058c372873ccbf6a98f7d32e9e544a6552b92123e4b0fb55"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDB9LCJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "data.aws_eks_cluster.eks-cluster",
            "data.kubectl_path_documents.docs"
          ]
        },
        {
          "index_key": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: grafana-import-dashboards\n  namespace: monitoring\n  labels:\n    app: grafana\n    component: import-dashboards\nspec:\n  template:\n    metadata:\n      name: grafana-import-dashboards\n      labels:\n        app: grafana\n        component: import-dashboards\n      annotations:\n        pod.beta.kubernetes.io/init-containers: '[\n          {\n            \"name\": \"wait-for-endpoints\",\n            \"image\": \"giantswarm/tiny-tools\",\n            \"imagePullPolicy\": \"IfNotPresent\",\n            \"command\": [\"fish\", \"-c\", \"echo \\\"waiting for endpoints...\\\"; while true; set endpoints (curl -s --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt --header \\\"Authorization: Bearer \\\"(cat /var/run/secrets/kubernetes.io/serviceaccount/token) https://kubernetes.default.svc/api/v1/namespaces/monitoring/endpoints/grafana); echo $endpoints | jq \\\".\\\"; if test (echo $endpoints | jq -r \\\".subsets[].addresses | length\\\") -gt 0; exit 0; end; echo \\\"waiting...\\\";sleep 1; end\"],\n            \"args\": [\"monitoring\", \"grafana\"]\n          }\n        ]'\n    spec:\n      containers:\n      - name: grafana-import-dashboards\n        image: giantswarm/tiny-tools\n        command: [\"/bin/sh\", \"-c\"]\n        workingDir: /opt/grafana-import-dashboards\n        args:\n          - \u003e\n            for file in *-datasource.json ; do\n              if [ -e \"$file\" ] ; then\n                echo \"importing $file\" \u0026\u0026\n                curl --silent --fail --show-error \\\n                  --request POST http://admin:admin@grafana/api/datasources \\\n                  --header \"Content-Type: application/json\" \\\n                  --header \"Accept: application/json\" \\\n                  --data-binary \"@$file\" ;\n                echo \"\" ;\n              fi\n            done ;\n            for file in *-dashboard.json ; do\n              if [ -e \"$file\" ] ; then\n                echo \"importing $file\" \u0026\u0026\n                curl --silent --fail --show-error \\\n                  --request POST http://admin:admin@grafana/api/dashboards/import \\\n                  --header \"Content-Type: application/json\" \\\n                  --header \"Accept: application/json\" \\\n                  --data-binary \"@$file\" ;\n                echo \"\" ;\n              fi\n            done ;\n        volumeMounts:\n        - name: config-volume\n          mountPath: /opt/grafana-import-dashboards\n      restartPolicy: Never\n      volumes:\n      - name: config-volume\n        configMap:\n          name: grafana-import-dashboards",
          "schema_version": 1,
          "attributes": {
            "api_version": "batch/v1",
            "apply_only": false,
            "force_conflicts": false,
            "force_new": false,
            "id": "/apis/batch/v1/namespaces/monitoring/jobs/grafana-import-dashboards",
            "ignore_fields": null,
            "kind": "Job",
            "live_manifest_incluster": "fc8a9b57cd9422cbd1ba33afc0a6c9bd4e1cf721c4280d79f35d9cc9f0e59367",
            "live_uid": "02f899af-8cc4-47c2-bdba-633b6a823bdd",
            "name": "grafana-import-dashboards",
            "namespace": "monitoring",
            "override_namespace": null,
            "sensitive_fields": null,
            "server_side_apply": false,
            "timeouts": null,
            "uid": "02f899af-8cc4-47c2-bdba-633b6a823bdd",
            "validate_schema": true,
            "wait": null,
            "wait_for_rollout": true,
            "yaml_body": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: grafana-import-dashboards\n  namespace: monitoring\n  labels:\n    app: grafana\n    component: import-dashboards\nspec:\n  template:\n    metadata:\n      name: grafana-import-dashboards\n      labels:\n        app: grafana\n        component: import-dashboards\n      annotations:\n        pod.beta.kubernetes.io/init-containers: '[\n          {\n            \"name\": \"wait-for-endpoints\",\n            \"image\": \"giantswarm/tiny-tools\",\n            \"imagePullPolicy\": \"IfNotPresent\",\n            \"command\": [\"fish\", \"-c\", \"echo \\\"waiting for endpoints...\\\"; while true; set endpoints (curl -s --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt --header \\\"Authorization: Bearer \\\"(cat /var/run/secrets/kubernetes.io/serviceaccount/token) https://kubernetes.default.svc/api/v1/namespaces/monitoring/endpoints/grafana); echo $endpoints | jq \\\".\\\"; if test (echo $endpoints | jq -r \\\".subsets[].addresses | length\\\") -gt 0; exit 0; end; echo \\\"waiting...\\\";sleep 1; end\"],\n            \"args\": [\"monitoring\", \"grafana\"]\n          }\n        ]'\n    spec:\n      containers:\n      - name: grafana-import-dashboards\n        image: giantswarm/tiny-tools\n        command: [\"/bin/sh\", \"-c\"]\n        workingDir: /opt/grafana-import-dashboards\n        args:\n          - \u003e\n            for file in *-datasource.json ; do\n              if [ -e \"$file\" ] ; then\n                echo \"importing $file\" \u0026\u0026\n                curl --silent --fail --show-error \\\n                  --request POST http://admin:admin@grafana/api/datasources \\\n                  --header \"Content-Type: application/json\" \\\n                  --header \"Accept: application/json\" \\\n                  --data-binary \"@$file\" ;\n                echo \"\" ;\n              fi\n            done ;\n            for file in *-dashboard.json ; do\n              if [ -e \"$file\" ] ; then\n                echo \"importing $file\" \u0026\u0026\n                curl --silent --fail --show-error \\\n                  --request POST http://admin:admin@grafana/api/dashboards/import \\\n                  --header \"Content-Type: application/json\" \\\n                  --header \"Accept: application/json\" \\\n                  --data-binary \"@$file\" ;\n                echo \"\" ;\n              fi\n            done ;\n        volumeMounts:\n        - name: config-volume\n          mountPath: /opt/grafana-import-dashboards\n      restartPolicy: Never\n      volumes:\n      - name: config-volume\n        configMap:\n          name: grafana-import-dashboards",
            "yaml_body_parsed": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: grafana\n    component: import-dashboards\n  name: grafana-import-dashboards\n  namespace: monitoring\nspec:\n  template:\n    metadata:\n      annotations:\n        pod.beta.kubernetes.io/init-containers: '[ { \"name\": \"wait-for-endpoints\",\n          \"image\": \"giantswarm/tiny-tools\", \"imagePullPolicy\": \"IfNotPresent\", \"command\":\n          [\"fish\", \"-c\", \"echo \\\"waiting for endpoints...\\\"; while true; set endpoints\n          (curl -s --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt --header\n          \\\"Authorization: Bearer \\\"(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\n          https://kubernetes.default.svc/api/v1/namespaces/monitoring/endpoints/grafana);\n          echo $endpoints | jq \\\".\\\"; if test (echo $endpoints | jq -r \\\".subsets[].addresses\n          | length\\\") -gt 0; exit 0; end; echo \\\"waiting...\\\";sleep 1; end\"], \"args\":\n          [\"monitoring\", \"grafana\"] } ]'\n      labels:\n        app: grafana\n        component: import-dashboards\n      name: grafana-import-dashboards\n    spec:\n      containers:\n      - args:\n        - |\n          for file in *-datasource.json ; do\n            if [ -e \"$file\" ] ; then\n              echo \"importing $file\" \u0026\u0026\n              curl --silent --fail --show-error \\\n                --request POST http://admin:admin@grafana/api/datasources \\\n                --header \"Content-Type: application/json\" \\\n                --header \"Accept: application/json\" \\\n                --data-binary \"@$file\" ;\n              echo \"\" ;\n            fi\n          done ; for file in *-dashboard.json ; do\n            if [ -e \"$file\" ] ; then\n              echo \"importing $file\" \u0026\u0026\n              curl --silent --fail --show-error \\\n                --request POST http://admin:admin@grafana/api/dashboards/import \\\n                --header \"Content-Type: application/json\" \\\n                --header \"Accept: application/json\" \\\n                --data-binary \"@$file\" ;\n              echo \"\" ;\n            fi\n          done ;\n        command:\n        - /bin/sh\n        - -c\n        image: giantswarm/tiny-tools\n        name: grafana-import-dashboards\n        volumeMounts:\n        - mountPath: /opt/grafana-import-dashboards\n          name: config-volume\n        workingDir: /opt/grafana-import-dashboards\n      restartPolicy: Never\n      volumes:\n      - configMap:\n          name: grafana-import-dashboards\n        name: config-volume\n",
            "yaml_incluster": "fc8a9b57cd9422cbd1ba33afc0a6c9bd4e1cf721c4280d79f35d9cc9f0e59367"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDB9LCJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "data.aws_eks_cluster.eks-cluster",
            "data.kubectl_path_documents.docs"
          ]
        },
        {
          "index_key": "apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: 2.1.0\n  name: kube-state-metrics\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - configmaps\n  - secrets\n  - nodes\n  - pods\n  - services\n  - resourcequotas\n  - replicationcontrollers\n  - limitranges\n  - persistentvolumeclaims\n  - persistentvolumes\n  - namespaces\n  - endpoints\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - apps\n  resources:\n  - statefulsets\n  - daemonsets\n  - deployments\n  - replicasets\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - batch\n  resources:\n  - cronjobs\n  - jobs\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - autoscaling\n  resources:\n  - horizontalpodautoscalers\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - authentication.k8s.io\n  resources:\n  - tokenreviews\n  verbs:\n  - create\n- apiGroups:\n  - authorization.k8s.io\n  resources:\n  - subjectaccessreviews\n  verbs:\n  - create\n- apiGroups:\n  - policy\n  resources:\n  - poddisruptionbudgets\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - certificates.k8s.io\n  resources:\n  - certificatesigningrequests\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - storage.k8s.io\n  resources:\n  - storageclasses\n  - volumeattachments\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - admissionregistration.k8s.io\n  resources:\n  - mutatingwebhookconfigurations\n  - validatingwebhookconfigurations\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - networkpolicies\n  - ingresses\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - coordination.k8s.io\n  resources:\n  - leases\n  verbs:\n  - list\n  - watch",
          "schema_version": 1,
          "attributes": {
            "api_version": "rbac.authorization.k8s.io/v1",
            "apply_only": false,
            "force_conflicts": false,
            "force_new": false,
            "id": "/apis/rbac.authorization.k8s.io/v1/clusterroles/kube-state-metrics",
            "ignore_fields": null,
            "kind": "ClusterRole",
            "live_manifest_incluster": "da27d17effd71d0b28376888625514216e9ec9e6f8b183d3a84c7d405b388d42",
            "live_uid": "b0a678f2-0df3-44b0-86eb-70616ae3e174",
            "name": "kube-state-metrics",
            "namespace": null,
            "override_namespace": null,
            "sensitive_fields": null,
            "server_side_apply": false,
            "timeouts": null,
            "uid": "b0a678f2-0df3-44b0-86eb-70616ae3e174",
            "validate_schema": true,
            "wait": null,
            "wait_for_rollout": true,
            "yaml_body": "apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: 2.1.0\n  name: kube-state-metrics\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - configmaps\n  - secrets\n  - nodes\n  - pods\n  - services\n  - resourcequotas\n  - replicationcontrollers\n  - limitranges\n  - persistentvolumeclaims\n  - persistentvolumes\n  - namespaces\n  - endpoints\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - apps\n  resources:\n  - statefulsets\n  - daemonsets\n  - deployments\n  - replicasets\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - batch\n  resources:\n  - cronjobs\n  - jobs\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - autoscaling\n  resources:\n  - horizontalpodautoscalers\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - authentication.k8s.io\n  resources:\n  - tokenreviews\n  verbs:\n  - create\n- apiGroups:\n  - authorization.k8s.io\n  resources:\n  - subjectaccessreviews\n  verbs:\n  - create\n- apiGroups:\n  - policy\n  resources:\n  - poddisruptionbudgets\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - certificates.k8s.io\n  resources:\n  - certificatesigningrequests\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - storage.k8s.io\n  resources:\n  - storageclasses\n  - volumeattachments\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - admissionregistration.k8s.io\n  resources:\n  - mutatingwebhookconfigurations\n  - validatingwebhookconfigurations\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - networkpolicies\n  - ingresses\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - coordination.k8s.io\n  resources:\n  - leases\n  verbs:\n  - list\n  - watch",
            "yaml_body_parsed": "apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: 2.1.0\n  name: kube-state-metrics\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - configmaps\n  - secrets\n  - nodes\n  - pods\n  - services\n  - resourcequotas\n  - replicationcontrollers\n  - limitranges\n  - persistentvolumeclaims\n  - persistentvolumes\n  - namespaces\n  - endpoints\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - apps\n  resources:\n  - statefulsets\n  - daemonsets\n  - deployments\n  - replicasets\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - batch\n  resources:\n  - cronjobs\n  - jobs\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - autoscaling\n  resources:\n  - horizontalpodautoscalers\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - authentication.k8s.io\n  resources:\n  - tokenreviews\n  verbs:\n  - create\n- apiGroups:\n  - authorization.k8s.io\n  resources:\n  - subjectaccessreviews\n  verbs:\n  - create\n- apiGroups:\n  - policy\n  resources:\n  - poddisruptionbudgets\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - certificates.k8s.io\n  resources:\n  - certificatesigningrequests\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - storage.k8s.io\n  resources:\n  - storageclasses\n  - volumeattachments\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - admissionregistration.k8s.io\n  resources:\n  - mutatingwebhookconfigurations\n  - validatingwebhookconfigurations\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - networkpolicies\n  - ingresses\n  verbs:\n  - list\n  - watch\n- apiGroups:\n  - coordination.k8s.io\n  resources:\n  - leases\n  verbs:\n  - list\n  - watch\n",
            "yaml_incluster": "da27d17effd71d0b28376888625514216e9ec9e6f8b183d3a84c7d405b388d42"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDB9LCJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "data.aws_eks_cluster.eks-cluster",
            "data.kubectl_path_documents.docs"
          ]
        },
        {
          "index_key": "apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: 2.1.0\n  name: kube-state-metrics\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: kube-state-metrics\nsubjects:\n- kind: ServiceAccount\n  name: kube-state-metrics\n  namespace: monitoring",
          "schema_version": 1,
          "attributes": {
            "api_version": "rbac.authorization.k8s.io/v1",
            "apply_only": false,
            "force_conflicts": false,
            "force_new": false,
            "id": "/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/kube-state-metrics",
            "ignore_fields": null,
            "kind": "ClusterRoleBinding",
            "live_manifest_incluster": "011a26c84f0fc07b93c7622ad2151dde6df79abd937c2f12fb73d62648c0a996",
            "live_uid": "931c55c9-739d-43f6-9ac3-4e4480c74dbd",
            "name": "kube-state-metrics",
            "namespace": null,
            "override_namespace": null,
            "sensitive_fields": null,
            "server_side_apply": false,
            "timeouts": null,
            "uid": "931c55c9-739d-43f6-9ac3-4e4480c74dbd",
            "validate_schema": true,
            "wait": null,
            "wait_for_rollout": true,
            "yaml_body": "apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: 2.1.0\n  name: kube-state-metrics\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: kube-state-metrics\nsubjects:\n- kind: ServiceAccount\n  name: kube-state-metrics\n  namespace: monitoring",
            "yaml_body_parsed": "apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: 2.1.0\n  name: kube-state-metrics\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: kube-state-metrics\nsubjects:\n- kind: ServiceAccount\n  name: kube-state-metrics\n  namespace: monitoring\n",
            "yaml_incluster": "011a26c84f0fc07b93c7622ad2151dde6df79abd937c2f12fb73d62648c0a996"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDB9LCJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "data.aws_eks_cluster.eks-cluster",
            "data.kubectl_path_documents.docs"
          ]
        },
        {
          "index_key": "apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: prometheus-configmap\n  namespace: monitoring\ndata:\n  prometheus.yml: |\n    global:\n      scrape_interval: 15s\n    rule_files:\n      - \"/etc/prometheus-rules/alert.rules\"\n    alerting:\n      alertmanagers:\n      - static_configs:\n        - targets:\n          - alertmanager:9093\n    scrape_configs:\n    - job_name: 'kubernetes-apiservers'\n      kubernetes_sd_configs:\n      - role: endpoints\n      scheme: https\n      tls_config:\n        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n      relabel_configs:\n      - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]\n        action: keep\n        regex: default;kubernetes;https\n    - job_name: 'kubernetes-nodes'\n      scheme: https\n      tls_config:\n        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n      kubernetes_sd_configs:\n      - role: node\n      relabel_configs:\n      - action: labelmap\n        regex: __meta_kubernetes_node_label_(.+)\n      - target_label: __address__\n        replacement: kubernetes.default.svc:443\n      - source_labels: [__meta_kubernetes_node_name]\n        regex: (.+)\n        target_label: __metrics_path__\n        replacement: /api/v1/nodes/1/proxy/metrics\n    - job_name: 'kubernetes-cadvisor'\n      scheme: https\n      tls_config:\n        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n      kubernetes_sd_configs:\n      - role: node\n      relabel_configs:\n      - action: labelmap\n        regex: __meta_kubernetes_node_label_(.+)\n      - target_label: __address__\n        replacement: kubernetes.default.svc:443\n      - source_labels: [__meta_kubernetes_node_name]\n        regex: (.+)\n        target_label: __metrics_path__\n        replacement: /api/v1/nodes/1/proxy/metrics/cadvisor\n    - job_name: 'kubernetes-service-endpoints'\n      kubernetes_sd_configs:\n      - role: endpoints\n      relabel_configs:\n      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]\n        action: keep\n        regex: true\n      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]\n        action: replace\n        target_label: __scheme__\n        regex: (https?)\n      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]\n        action: replace\n        target_label: __metrics_path__\n        regex: (.+)\n      - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]\n        action: replace\n        target_label: __address__\n        regex: ([^:]+)(?::\\d+)?;(\\d+)\n        replacement: $1:$2\n      - action: labelmap\n        regex: __meta_kubernetes_service_label_(.+)\n      - source_labels: [__meta_kubernetes_namespace]\n        action: replace\n        target_label: kubernetes_namespace\n      - source_labels: [__meta_kubernetes_service_name]\n        action: replace\n        target_label: kubernetes_name\n    - job_name: 'kubernetes-services'\n      metrics_path: /probe\n      params:\n        module: [http_2xx]\n      kubernetes_sd_configs:\n      - role: service\n      relabel_configs:\n      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]\n        action: keep\n        regex: true\n      - source_labels: [__address__]\n        target_label: __param_target\n      - target_label: __address__\n        replacement: blackbox-exporter.example.com:9115\n      - source_labels: [__param_target]\n        target_label: instance\n      - action: labelmap\n        regex: __meta_kubernetes_service_label_(.+)\n      - source_labels: [__meta_kubernetes_namespace]\n        target_label: kubernetes_namespace\n      - source_labels: [__meta_kubernetes_service_name]\n        target_label: kubernetes_name\n    - job_name: 'kubernetes-pods'\n      kubernetes_sd_configs:\n      - role: pod\n      relabel_configs:\n      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]\n        action: keep\n        regex: true\n      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]\n        action: replace\n        target_label: __metrics_path__\n        regex: (.+)\n      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]\n        action: replace\n        regex: ([^:]+)(?::\\d+)?;(\\d+)\n        replacement: $1:$2\n        target_label: __address__\n      - action: labelmap\n        regex: __meta_kubernetes_pod_label_(.+)\n      - source_labels: [__meta_kubernetes_namespace]\n        action: replace\n        target_label: kubernetes_namespace\n      - source_labels: [__meta_kubernetes_pod_name]\n        action: replace\n        target_label: kubernetes_pod_name\n    - job_name: 'kubernetes-ingresses'\n      kubernetes_sd_configs:\n      - role: ingress\n      relabel_configs:\n      - source_labels: [__meta_kubernetes_ingress_annotation_prometheus_io_probe]\n        action: keep\n        regex: true\n      - source_labels: [__meta_kubernetes_ingress_scheme,__address__,__meta_kubernetes_ingress_path]\n        regex: (.+);(.+);(.+)\n        replacement: 1://23\n        target_label: __param_target\n      - target_label: __address__\n        replacement: blackbox-exporter.example.com:9115\n      - source_labels: [__param_target]\n        target_label: instance\n      - action: labelmap\n        regex: __meta_kubernetes_ingress_label_(.+)\n      - source_labels: [__meta_kubernetes_namespace]\n        target_label: kubernetes_namespace\n      - source_labels: [__meta_kubernetes_ingress_name]\n        target_label: kubernetes_name\n    - job_name: 'kube-state-metrics'\n      static_configs:\n        - targets: ['kube-state-metrics.monitoring.svc.cluster.local:8080']\n    - job_name: 'node-exporter'\n      kubernetes_sd_configs:\n      - role: endpoints\n      relabel_configs:\n      - source_labels: [__meta_kubernetes_endpoints_name]\n        regex: 'node-exporter'\n        action: keep",
          "schema_version": 1,
          "attributes": {
            "api_version": "v1",
            "apply_only": false,
            "force_conflicts": false,
            "force_new": false,
            "id": "/api/v1/namespaces/monitoring/configmaps/prometheus-configmap",
            "ignore_fields": null,
            "kind": "ConfigMap",
            "live_manifest_incluster": "d864c129a01f8dc63899340b6314dce8674e21252beef18b1db335b02475913b",
            "live_uid": "7b20c29a-4150-4d93-8d9f-c7995e68cfcb",
            "name": "prometheus-configmap",
            "namespace": "monitoring",
            "override_namespace": null,
            "sensitive_fields": null,
            "server_side_apply": false,
            "timeouts": null,
            "uid": "7b20c29a-4150-4d93-8d9f-c7995e68cfcb",
            "validate_schema": true,
            "wait": null,
            "wait_for_rollout": true,
            "yaml_body": "apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: prometheus-configmap\n  namespace: monitoring\ndata:\n  prometheus.yml: |\n    global:\n      scrape_interval: 15s\n    rule_files:\n      - \"/etc/prometheus-rules/alert.rules\"\n    alerting:\n      alertmanagers:\n      - static_configs:\n        - targets:\n          - alertmanager:9093\n    scrape_configs:\n    - job_name: 'kubernetes-apiservers'\n      kubernetes_sd_configs:\n      - role: endpoints\n      scheme: https\n      tls_config:\n        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n      relabel_configs:\n      - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]\n        action: keep\n        regex: default;kubernetes;https\n    - job_name: 'kubernetes-nodes'\n      scheme: https\n      tls_config:\n        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n      kubernetes_sd_configs:\n      - role: node\n      relabel_configs:\n      - action: labelmap\n        regex: __meta_kubernetes_node_label_(.+)\n      - target_label: __address__\n        replacement: kubernetes.default.svc:443\n      - source_labels: [__meta_kubernetes_node_name]\n        regex: (.+)\n        target_label: __metrics_path__\n        replacement: /api/v1/nodes/1/proxy/metrics\n    - job_name: 'kubernetes-cadvisor'\n      scheme: https\n      tls_config:\n        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n      kubernetes_sd_configs:\n      - role: node\n      relabel_configs:\n      - action: labelmap\n        regex: __meta_kubernetes_node_label_(.+)\n      - target_label: __address__\n        replacement: kubernetes.default.svc:443\n      - source_labels: [__meta_kubernetes_node_name]\n        regex: (.+)\n        target_label: __metrics_path__\n        replacement: /api/v1/nodes/1/proxy/metrics/cadvisor\n    - job_name: 'kubernetes-service-endpoints'\n      kubernetes_sd_configs:\n      - role: endpoints\n      relabel_configs:\n      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]\n        action: keep\n        regex: true\n      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]\n        action: replace\n        target_label: __scheme__\n        regex: (https?)\n      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]\n        action: replace\n        target_label: __metrics_path__\n        regex: (.+)\n      - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]\n        action: replace\n        target_label: __address__\n        regex: ([^:]+)(?::\\d+)?;(\\d+)\n        replacement: $1:$2\n      - action: labelmap\n        regex: __meta_kubernetes_service_label_(.+)\n      - source_labels: [__meta_kubernetes_namespace]\n        action: replace\n        target_label: kubernetes_namespace\n      - source_labels: [__meta_kubernetes_service_name]\n        action: replace\n        target_label: kubernetes_name\n    - job_name: 'kubernetes-services'\n      metrics_path: /probe\n      params:\n        module: [http_2xx]\n      kubernetes_sd_configs:\n      - role: service\n      relabel_configs:\n      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]\n        action: keep\n        regex: true\n      - source_labels: [__address__]\n        target_label: __param_target\n      - target_label: __address__\n        replacement: blackbox-exporter.example.com:9115\n      - source_labels: [__param_target]\n        target_label: instance\n      - action: labelmap\n        regex: __meta_kubernetes_service_label_(.+)\n      - source_labels: [__meta_kubernetes_namespace]\n        target_label: kubernetes_namespace\n      - source_labels: [__meta_kubernetes_service_name]\n        target_label: kubernetes_name\n    - job_name: 'kubernetes-pods'\n      kubernetes_sd_configs:\n      - role: pod\n      relabel_configs:\n      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]\n        action: keep\n        regex: true\n      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]\n        action: replace\n        target_label: __metrics_path__\n        regex: (.+)\n      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]\n        action: replace\n        regex: ([^:]+)(?::\\d+)?;(\\d+)\n        replacement: $1:$2\n        target_label: __address__\n      - action: labelmap\n        regex: __meta_kubernetes_pod_label_(.+)\n      - source_labels: [__meta_kubernetes_namespace]\n        action: replace\n        target_label: kubernetes_namespace\n      - source_labels: [__meta_kubernetes_pod_name]\n        action: replace\n        target_label: kubernetes_pod_name\n    - job_name: 'kubernetes-ingresses'\n      kubernetes_sd_configs:\n      - role: ingress\n      relabel_configs:\n      - source_labels: [__meta_kubernetes_ingress_annotation_prometheus_io_probe]\n        action: keep\n        regex: true\n      - source_labels: [__meta_kubernetes_ingress_scheme,__address__,__meta_kubernetes_ingress_path]\n        regex: (.+);(.+);(.+)\n        replacement: 1://23\n        target_label: __param_target\n      - target_label: __address__\n        replacement: blackbox-exporter.example.com:9115\n      - source_labels: [__param_target]\n        target_label: instance\n      - action: labelmap\n        regex: __meta_kubernetes_ingress_label_(.+)\n      - source_labels: [__meta_kubernetes_namespace]\n        target_label: kubernetes_namespace\n      - source_labels: [__meta_kubernetes_ingress_name]\n        target_label: kubernetes_name\n    - job_name: 'kube-state-metrics'\n      static_configs:\n        - targets: ['kube-state-metrics.monitoring.svc.cluster.local:8080']\n    - job_name: 'node-exporter'\n      kubernetes_sd_configs:\n      - role: endpoints\n      relabel_configs:\n      - source_labels: [__meta_kubernetes_endpoints_name]\n        regex: 'node-exporter'\n        action: keep",
            "yaml_body_parsed": "apiVersion: v1\ndata:\n  prometheus.yml: |-\n    global:\n      scrape_interval: 15s\n    rule_files:\n      - \"/etc/prometheus-rules/alert.rules\"\n    alerting:\n      alertmanagers:\n      - static_configs:\n        - targets:\n          - alertmanager:9093\n    scrape_configs:\n    - job_name: 'kubernetes-apiservers'\n      kubernetes_sd_configs:\n      - role: endpoints\n      scheme: https\n      tls_config:\n        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n      relabel_configs:\n      - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]\n        action: keep\n        regex: default;kubernetes;https\n    - job_name: 'kubernetes-nodes'\n      scheme: https\n      tls_config:\n        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n      kubernetes_sd_configs:\n      - role: node\n      relabel_configs:\n      - action: labelmap\n        regex: __meta_kubernetes_node_label_(.+)\n      - target_label: __address__\n        replacement: kubernetes.default.svc:443\n      - source_labels: [__meta_kubernetes_node_name]\n        regex: (.+)\n        target_label: __metrics_path__\n        replacement: /api/v1/nodes/1/proxy/metrics\n    - job_name: 'kubernetes-cadvisor'\n      scheme: https\n      tls_config:\n        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n      kubernetes_sd_configs:\n      - role: node\n      relabel_configs:\n      - action: labelmap\n        regex: __meta_kubernetes_node_label_(.+)\n      - target_label: __address__\n        replacement: kubernetes.default.svc:443\n      - source_labels: [__meta_kubernetes_node_name]\n        regex: (.+)\n        target_label: __metrics_path__\n        replacement: /api/v1/nodes/1/proxy/metrics/cadvisor\n    - job_name: 'kubernetes-service-endpoints'\n      kubernetes_sd_configs:\n      - role: endpoints\n      relabel_configs:\n      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]\n        action: keep\n        regex: true\n      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]\n        action: replace\n        target_label: __scheme__\n        regex: (https?)\n      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]\n        action: replace\n        target_label: __metrics_path__\n        regex: (.+)\n      - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]\n        action: replace\n        target_label: __address__\n        regex: ([^:]+)(?::\\d+)?;(\\d+)\n        replacement: $1:$2\n      - action: labelmap\n        regex: __meta_kubernetes_service_label_(.+)\n      - source_labels: [__meta_kubernetes_namespace]\n        action: replace\n        target_label: kubernetes_namespace\n      - source_labels: [__meta_kubernetes_service_name]\n        action: replace\n        target_label: kubernetes_name\n    - job_name: 'kubernetes-services'\n      metrics_path: /probe\n      params:\n        module: [http_2xx]\n      kubernetes_sd_configs:\n      - role: service\n      relabel_configs:\n      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]\n        action: keep\n        regex: true\n      - source_labels: [__address__]\n        target_label: __param_target\n      - target_label: __address__\n        replacement: blackbox-exporter.example.com:9115\n      - source_labels: [__param_target]\n        target_label: instance\n      - action: labelmap\n        regex: __meta_kubernetes_service_label_(.+)\n      - source_labels: [__meta_kubernetes_namespace]\n        target_label: kubernetes_namespace\n      - source_labels: [__meta_kubernetes_service_name]\n        target_label: kubernetes_name\n    - job_name: 'kubernetes-pods'\n      kubernetes_sd_configs:\n      - role: pod\n      relabel_configs:\n      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]\n        action: keep\n        regex: true\n      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]\n        action: replace\n        target_label: __metrics_path__\n        regex: (.+)\n      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]\n        action: replace\n        regex: ([^:]+)(?::\\d+)?;(\\d+)\n        replacement: $1:$2\n        target_label: __address__\n      - action: labelmap\n        regex: __meta_kubernetes_pod_label_(.+)\n      - source_labels: [__meta_kubernetes_namespace]\n        action: replace\n        target_label: kubernetes_namespace\n      - source_labels: [__meta_kubernetes_pod_name]\n        action: replace\n        target_label: kubernetes_pod_name\n    - job_name: 'kubernetes-ingresses'\n      kubernetes_sd_configs:\n      - role: ingress\n      relabel_configs:\n      - source_labels: [__meta_kubernetes_ingress_annotation_prometheus_io_probe]\n        action: keep\n        regex: true\n      - source_labels: [__meta_kubernetes_ingress_scheme,__address__,__meta_kubernetes_ingress_path]\n        regex: (.+);(.+);(.+)\n        replacement: 1://23\n        target_label: __param_target\n      - target_label: __address__\n        replacement: blackbox-exporter.example.com:9115\n      - source_labels: [__param_target]\n        target_label: instance\n      - action: labelmap\n        regex: __meta_kubernetes_ingress_label_(.+)\n      - source_labels: [__meta_kubernetes_namespace]\n        target_label: kubernetes_namespace\n      - source_labels: [__meta_kubernetes_ingress_name]\n        target_label: kubernetes_name\n    - job_name: 'kube-state-metrics'\n      static_configs:\n        - targets: ['kube-state-metrics.monitoring.svc.cluster.local:8080']\n    - job_name: 'node-exporter'\n      kubernetes_sd_configs:\n      - role: endpoints\n      relabel_configs:\n      - source_labels: [__meta_kubernetes_endpoints_name]\n        regex: 'node-exporter'\n        action: keep\nkind: ConfigMap\nmetadata:\n  name: prometheus-configmap\n  namespace: monitoring\n",
            "yaml_incluster": "d864c129a01f8dc63899340b6314dce8674e21252beef18b1db335b02475913b"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDB9LCJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "data.aws_eks_cluster.eks-cluster",
            "data.kubectl_path_documents.docs"
          ]
        },
        {
          "index_key": "apiVersion: v1\nkind: Namespace\nmetadata:\n  name: monitoring",
          "schema_version": 1,
          "attributes": {
            "api_version": "v1",
            "apply_only": false,
            "force_conflicts": false,
            "force_new": false,
            "id": "/api/v1/namespaces/monitoring",
            "ignore_fields": null,
            "kind": "Namespace",
            "live_manifest_incluster": "f69d26eb9fda86d8285fbe43dacc4b50d09693b98f13e1cbd8abee001b32c1ae",
            "live_uid": "f870c34f-2934-4b9c-9746-807e58a1dfd4",
            "name": "monitoring",
            "namespace": null,
            "override_namespace": null,
            "sensitive_fields": null,
            "server_side_apply": false,
            "timeouts": null,
            "uid": "f870c34f-2934-4b9c-9746-807e58a1dfd4",
            "validate_schema": true,
            "wait": null,
            "wait_for_rollout": true,
            "yaml_body": "apiVersion: v1\nkind: Namespace\nmetadata:\n  name: monitoring",
            "yaml_body_parsed": "apiVersion: v1\nkind: Namespace\nmetadata:\n  name: monitoring\n",
            "yaml_incluster": "f69d26eb9fda86d8285fbe43dacc4b50d09693b98f13e1cbd8abee001b32c1ae"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDB9LCJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "data.aws_eks_cluster.eks-cluster",
            "data.kubectl_path_documents.docs"
          ]
        },
        {
          "index_key": "apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    prometheus.io/scrape: 'true'\n  labels:\n    name: prometheus\n  name: prometheus\n  namespace: monitoring\nspec:\n  selector:\n    app: prometheus\n  type: LoadBalancer\n  ports:\n  - name: prometheus\n    protocol: TCP\n    port: 9090\n    targetPort: 9090",
          "schema_version": 1,
          "attributes": {
            "api_version": "v1",
            "apply_only": false,
            "force_conflicts": false,
            "force_new": false,
            "id": "/api/v1/namespaces/monitoring/services/prometheus",
            "ignore_fields": null,
            "kind": "Service",
            "live_manifest_incluster": "14a615a8b510461742e370998d91466aadb7505a0cbf098bb909c8ebe7ef7be9",
            "live_uid": "121bf830-ac27-4988-a003-88341facc96f",
            "name": "prometheus",
            "namespace": "monitoring",
            "override_namespace": null,
            "sensitive_fields": null,
            "server_side_apply": false,
            "timeouts": null,
            "uid": "121bf830-ac27-4988-a003-88341facc96f",
            "validate_schema": true,
            "wait": null,
            "wait_for_rollout": true,
            "yaml_body": "apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    prometheus.io/scrape: 'true'\n  labels:\n    name: prometheus\n  name: prometheus\n  namespace: monitoring\nspec:\n  selector:\n    app: prometheus\n  type: LoadBalancer\n  ports:\n  - name: prometheus\n    protocol: TCP\n    port: 9090\n    targetPort: 9090",
            "yaml_body_parsed": "apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    prometheus.io/scrape: \"true\"\n  labels:\n    name: prometheus\n  name: prometheus\n  namespace: monitoring\nspec:\n  ports:\n  - name: prometheus\n    port: 9090\n    protocol: TCP\n    targetPort: 9090\n  selector:\n    app: prometheus\n  type: LoadBalancer\n",
            "yaml_incluster": "14a615a8b510461742e370998d91466aadb7505a0cbf098bb909c8ebe7ef7be9"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDB9LCJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "data.aws_eks_cluster.eks-cluster",
            "data.kubectl_path_documents.docs"
          ]
        },
        {
          "index_key": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/component: exporter\n    app.kubernetes.io/name: node-exporter\n  name: node-exporter\n  namespace: monitoring\nspec:\n  clusterIP: None\n  ports:\n  - name: http\n    port: 9100\n    targetPort: 9100\n  selector:\n    app.kubernetes.io/component: exporter\n    app.kubernetes.io/name: node-exporter",
          "schema_version": 1,
          "attributes": {
            "api_version": "v1",
            "apply_only": false,
            "force_conflicts": false,
            "force_new": false,
            "id": "/api/v1/namespaces/monitoring/services/node-exporter",
            "ignore_fields": null,
            "kind": "Service",
            "live_manifest_incluster": "70557a9deabf0abf2afdbbc4c4709e13702a6557b77781e17165df1c10cf3d9a",
            "live_uid": "0688df13-dc66-41bd-81e8-1c1231c4d74b",
            "name": "node-exporter",
            "namespace": "monitoring",
            "override_namespace": null,
            "sensitive_fields": null,
            "server_side_apply": false,
            "timeouts": null,
            "uid": "0688df13-dc66-41bd-81e8-1c1231c4d74b",
            "validate_schema": true,
            "wait": null,
            "wait_for_rollout": true,
            "yaml_body": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/component: exporter\n    app.kubernetes.io/name: node-exporter\n  name: node-exporter\n  namespace: monitoring\nspec:\n  clusterIP: None\n  ports:\n  - name: http\n    port: 9100\n    targetPort: 9100\n  selector:\n    app.kubernetes.io/component: exporter\n    app.kubernetes.io/name: node-exporter",
            "yaml_body_parsed": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/component: exporter\n    app.kubernetes.io/name: node-exporter\n  name: node-exporter\n  namespace: monitoring\nspec:\n  clusterIP: None\n  ports:\n  - name: http\n    port: 9100\n    targetPort: 9100\n  selector:\n    app.kubernetes.io/component: exporter\n    app.kubernetes.io/name: node-exporter\n",
            "yaml_incluster": "70557a9deabf0abf2afdbbc4c4709e13702a6557b77781e17165df1c10cf3d9a"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDB9LCJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "data.aws_eks_cluster.eks-cluster",
            "data.kubectl_path_documents.docs"
          ]
        },
        {
          "index_key": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: 2.1.0\n  name: kube-state-metrics\n  namespace: monitoring\nspec:\n  clusterIP: None\n  ports:\n  - name: http-metrics\n    port: 8080\n    targetPort: http-metrics\n  - name: telemetry\n    port: 8081\n    targetPort: telemetry\n  selector:\n    app.kubernetes.io/name: kube-state-metrics",
          "schema_version": 1,
          "attributes": {
            "api_version": "v1",
            "apply_only": false,
            "force_conflicts": false,
            "force_new": false,
            "id": "/api/v1/namespaces/monitoring/services/kube-state-metrics",
            "ignore_fields": null,
            "kind": "Service",
            "live_manifest_incluster": "984d1fb92d84c2099413a83409521f9d54e375e5aed16a492357e0c17da0db71",
            "live_uid": "4427be73-15c3-4fa4-a41e-287ce2367d02",
            "name": "kube-state-metrics",
            "namespace": "monitoring",
            "override_namespace": null,
            "sensitive_fields": null,
            "server_side_apply": false,
            "timeouts": null,
            "uid": "4427be73-15c3-4fa4-a41e-287ce2367d02",
            "validate_schema": true,
            "wait": null,
            "wait_for_rollout": true,
            "yaml_body": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: 2.1.0\n  name: kube-state-metrics\n  namespace: monitoring\nspec:\n  clusterIP: None\n  ports:\n  - name: http-metrics\n    port: 8080\n    targetPort: http-metrics\n  - name: telemetry\n    port: 8081\n    targetPort: telemetry\n  selector:\n    app.kubernetes.io/name: kube-state-metrics",
            "yaml_body_parsed": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: 2.1.0\n  name: kube-state-metrics\n  namespace: monitoring\nspec:\n  clusterIP: None\n  ports:\n  - name: http-metrics\n    port: 8080\n    targetPort: http-metrics\n  - name: telemetry\n    port: 8081\n    targetPort: telemetry\n  selector:\n    app.kubernetes.io/name: kube-state-metrics\n",
            "yaml_incluster": "984d1fb92d84c2099413a83409521f9d54e375e5aed16a492357e0c17da0db71"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDB9LCJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "data.aws_eks_cluster.eks-cluster",
            "data.kubectl_path_documents.docs"
          ]
        },
        {
          "index_key": "apiVersion: v1\nkind: Service\nmetadata:\n  name: grafana\n  namespace: monitoring\n  labels:\n    app: grafana\n    component: core\nspec:\n  type: LoadBalancer\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 3000\n  selector:\n    app: grafana\n    component: core",
          "schema_version": 1,
          "attributes": {
            "api_version": "v1",
            "apply_only": false,
            "force_conflicts": false,
            "force_new": false,
            "id": "/api/v1/namespaces/monitoring/services/grafana",
            "ignore_fields": null,
            "kind": "Service",
            "live_manifest_incluster": "2f2c99b1df146270df347e1d4ae5a6b585b322cec46d15bdbcb19b86ce933ba7",
            "live_uid": "4598571a-2eb6-4f35-8529-3561cc57c0e7",
            "name": "grafana",
            "namespace": "monitoring",
            "override_namespace": null,
            "sensitive_fields": null,
            "server_side_apply": false,
            "timeouts": null,
            "uid": "4598571a-2eb6-4f35-8529-3561cc57c0e7",
            "validate_schema": true,
            "wait": null,
            "wait_for_rollout": true,
            "yaml_body": "apiVersion: v1\nkind: Service\nmetadata:\n  name: grafana\n  namespace: monitoring\n  labels:\n    app: grafana\n    component: core\nspec:\n  type: LoadBalancer\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 3000\n  selector:\n    app: grafana\n    component: core",
            "yaml_body_parsed": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: grafana\n    component: core\n  name: grafana\n  namespace: monitoring\nspec:\n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 3000\n  selector:\n    app: grafana\n    component: core\n  type: LoadBalancer\n",
            "yaml_incluster": "2f2c99b1df146270df347e1d4ae5a6b585b322cec46d15bdbcb19b86ce933ba7"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDB9LCJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "data.aws_eks_cluster.eks-cluster",
            "data.kubectl_path_documents.docs"
          ]
        },
        {
          "index_key": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    app.kubernetes.io/component: exporter\n    app.kubernetes.io/name: node-exporter\n  name: node-exporter\n  namespace: monitoring",
          "schema_version": 1,
          "attributes": {
            "api_version": "v1",
            "apply_only": false,
            "force_conflicts": false,
            "force_new": false,
            "id": "/api/v1/namespaces/monitoring/serviceaccounts/node-exporter",
            "ignore_fields": null,
            "kind": "ServiceAccount",
            "live_manifest_incluster": "8215266e2ab6d698c0a593f75af149ed7bddc5fd5f94a0b326e6dc60bbbb0a8a",
            "live_uid": "2a7b226a-4b2f-4483-9fea-adc4acedd906",
            "name": "node-exporter",
            "namespace": "monitoring",
            "override_namespace": null,
            "sensitive_fields": null,
            "server_side_apply": false,
            "timeouts": null,
            "uid": "2a7b226a-4b2f-4483-9fea-adc4acedd906",
            "validate_schema": true,
            "wait": null,
            "wait_for_rollout": true,
            "yaml_body": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    app.kubernetes.io/component: exporter\n    app.kubernetes.io/name: node-exporter\n  name: node-exporter\n  namespace: monitoring",
            "yaml_body_parsed": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    app.kubernetes.io/component: exporter\n    app.kubernetes.io/name: node-exporter\n  name: node-exporter\n  namespace: monitoring\n",
            "yaml_incluster": "8215266e2ab6d698c0a593f75af149ed7bddc5fd5f94a0b326e6dc60bbbb0a8a"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDB9LCJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "data.aws_eks_cluster.eks-cluster",
            "data.kubectl_path_documents.docs"
          ]
        },
        {
          "index_key": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: 2.1.0\n  name: kube-state-metrics\n  namespace: monitoring",
          "schema_version": 1,
          "attributes": {
            "api_version": "v1",
            "apply_only": false,
            "force_conflicts": false,
            "force_new": false,
            "id": "/api/v1/namespaces/monitoring/serviceaccounts/kube-state-metrics",
            "ignore_fields": null,
            "kind": "ServiceAccount",
            "live_manifest_incluster": "b6534a0f915d05dbfa80742f90647d824d29e843fb7795e3207b03888767783e",
            "live_uid": "9a657dcc-5ae5-43eb-bc90-12a6cc7de78f",
            "name": "kube-state-metrics",
            "namespace": "monitoring",
            "override_namespace": null,
            "sensitive_fields": null,
            "server_side_apply": false,
            "timeouts": null,
            "uid": "9a657dcc-5ae5-43eb-bc90-12a6cc7de78f",
            "validate_schema": true,
            "wait": null,
            "wait_for_rollout": true,
            "yaml_body": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: 2.1.0\n  name: kube-state-metrics\n  namespace: monitoring",
            "yaml_body_parsed": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: 2.1.0\n  name: kube-state-metrics\n  namespace: monitoring\n",
            "yaml_incluster": "b6534a0f915d05dbfa80742f90647d824d29e843fb7795e3207b03888767783e"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDB9LCJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "data.aws_eks_cluster.eks-cluster",
            "data.kubectl_path_documents.docs"
          ]
        }
      ]
    }
  ],
  "check_results": null
}
